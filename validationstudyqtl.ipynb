{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"widgets":{"application/vnd.jupyter.widget-state+json":{"0b62f9148cdf40749d6e040cab580bc0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"0bb14d32242a4e96a3e1dc6880d1c263":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4f3d180b99fb452683cf642280181105","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_d2d608fc3e734af49e6540147d2d5574","value":548105171}},"0e84b3b8bfcc4f6882cbb669bc6cf565":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0eb671d5cbf64111bf289943b0219d1e":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"12a0a7c385a64c8088cece9de6ba35ec":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_be0f325ebb744823b42a747052a04c6a","placeholder":"​","style":"IPY_MODEL_7fc9fbe3b6b84c9d8877fc1bc31c67fa","value":" 26.0/26.0 [00:00&lt;00:00, 470B/s]"}},"13e72aa64a774f858c341ad0d7e83bbb":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14b1ff82d2474050a4c053bcf68d5fcf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1ba7b7c1d97c4358972ba8154dac9397":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8139fc2c958f409393d6f838ea89d83b","placeholder":"​","style":"IPY_MODEL_632d2ebb311141198cd5f12f9b7696e1","value":" 1.36M/1.36M [00:00&lt;00:00, 5.55MB/s]"}},"2979a0b3fc264fdd82ed8f113e7e7bf4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_82fc757c86a742c885581e38483bae69","IPY_MODEL_f7cdcfe516694ebab924e025b77d608f","IPY_MODEL_89b1bc4f8897406eb79d22d9ee3c858f"],"layout":"IPY_MODEL_959881fe43fb4c15b798b42f6b7bd7a5"}},"2d99ca64315b42edb205e9b3a46e60bd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c38ecf9664c8488480606e3702eb325d","placeholder":"​","style":"IPY_MODEL_d21d6af1d5ef439f8840c1e806d5d688","value":" 665/665 [00:00&lt;00:00, 11.0kB/s]"}},"329fda7ab3fa4e3bbd956bd60a697a60":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_cd43e94351fd45f290493b02e7da2dc7","IPY_MODEL_745eeb12595d419687f39f791070d119","IPY_MODEL_fa45d12b6ab74135b97987b499dc8f52"],"layout":"IPY_MODEL_dd4fd245465b49efa0dbf2e311a87c29"}},"335f319e67c846d991182c5762dd275f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"34efd389c8bd416c9dfeca26e3ee1acc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"35206cfc26d54cbbb8c00168fb709df8":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d50fa1618e9b41808e660e8ee1294be0","placeholder":"​","style":"IPY_MODEL_d406a9bb0a7340b5bb5647727dde1688","value":" 548M/548M [00:06&lt;00:00, 115MB/s]"}},"3cf0ed33a09e4ea5b683b928257767c0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"44e26082a459439ab15c52186951382e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_335f319e67c846d991182c5762dd275f","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e2bd641b7cf644c6add67d418862d101","value":26}},"4aac5ed370cb4a508ca47e5f3f62f6de":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_887e0f0c21d8492e82a9c2fd4d2370e0","placeholder":"​","style":"IPY_MODEL_97da49950c924263861b9d5e7824e77e","value":"tokenizer.json: 100%"}},"4c972e2657684a7aa0af83b2225c3f40":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4f3d180b99fb452683cf642280181105":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59439f0d8ecb4d26bc39f0271a23a2b5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ee2cfc7ffae43aa94c1f8694f39a6d8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"632d2ebb311141198cd5f12f9b7696e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"66f7226a40b240b4a3641d635a2ef53f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3cf0ed33a09e4ea5b683b928257767c0","placeholder":"​","style":"IPY_MODEL_a44dbe35423e41a5b632d5cbfc298b93","value":"tokenizer_config.json: 100%"}},"6d56b50904fa499d96904504e8ee9378":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_14b1ff82d2474050a4c053bcf68d5fcf","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_0b62f9148cdf40749d6e040cab580bc0","value":665}},"6fe6435787924e94b4753219a946ea3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4aac5ed370cb4a508ca47e5f3f62f6de","IPY_MODEL_f10b7f60c3e94be5b0d77bf41380c333","IPY_MODEL_1ba7b7c1d97c4358972ba8154dac9397"],"layout":"IPY_MODEL_0eb671d5cbf64111bf289943b0219d1e"}},"745eeb12595d419687f39f791070d119":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c3143c4da1294be1b39ce7bf4076c487","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c3589fb8fcd042a891ec6bb55189a175","value":456318}},"757e398c524b46febb6b26ca67604f6a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_eb1426c802cf4d72abda9d5653c2b2f8","placeholder":"​","style":"IPY_MODEL_a3ff0e0e9fa54706a95144e51513b839","value":"model.safetensors: 100%"}},"778959a7098d49aea0fb9e48003e3559":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_757e398c524b46febb6b26ca67604f6a","IPY_MODEL_0bb14d32242a4e96a3e1dc6880d1c263","IPY_MODEL_35206cfc26d54cbbb8c00168fb709df8"],"layout":"IPY_MODEL_e9937166be26402cae13c6f03391ab17"}},"7fc9fbe3b6b84c9d8877fc1bc31c67fa":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8139fc2c958f409393d6f838ea89d83b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82fc757c86a742c885581e38483bae69":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_a4107e64dade46c4bff304d3e2dc040f","placeholder":"​","style":"IPY_MODEL_9ed46a5c274640dd99b927466e82fe41","value":"vocab.json: 100%"}},"847c9c8f934e40b7815444deca13984f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"887e0f0c21d8492e82a9c2fd4d2370e0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"89b1bc4f8897406eb79d22d9ee3c858f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13e72aa64a774f858c341ad0d7e83bbb","placeholder":"​","style":"IPY_MODEL_fae773309b3a4a98ab1444444bb2c4a5","value":" 1.04M/1.04M [00:00&lt;00:00, 4.18MB/s]"}},"9349a071a39d4715b9f8872b38a83107":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"959881fe43fb4c15b798b42f6b7bd7a5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"97da49950c924263861b9d5e7824e77e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b2ba5ee692340a5b7abef958d93b663":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ed46a5c274640dd99b927466e82fe41":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3ff0e0e9fa54706a95144e51513b839":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a4107e64dade46c4bff304d3e2dc040f":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a44dbe35423e41a5b632d5cbfc298b93":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a86e0d751101440d9f3e7a02daef73e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a87511ad7adb4190a8be77f563495afc":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d349529deb68438ead35c2c7bfc4ed8a","IPY_MODEL_6d56b50904fa499d96904504e8ee9378","IPY_MODEL_2d99ca64315b42edb205e9b3a46e60bd"],"layout":"IPY_MODEL_5ee2cfc7ffae43aa94c1f8694f39a6d8"}},"adc807a87e4746459e6534b965c669c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bdb5975920d14edeb32f640f3a572c68":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"be0f325ebb744823b42a747052a04c6a":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3143c4da1294be1b39ce7bf4076c487":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c3589fb8fcd042a891ec6bb55189a175":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c38ecf9664c8488480606e3702eb325d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cd43e94351fd45f290493b02e7da2dc7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0e84b3b8bfcc4f6882cbb669bc6cf565","placeholder":"​","style":"IPY_MODEL_a86e0d751101440d9f3e7a02daef73e4","value":"merges.txt: 100%"}},"d21d6af1d5ef439f8840c1e806d5d688":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2d608fc3e734af49e6540147d2d5574":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d349529deb68438ead35c2c7bfc4ed8a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bdb5975920d14edeb32f640f3a572c68","placeholder":"​","style":"IPY_MODEL_4c972e2657684a7aa0af83b2225c3f40","value":"config.json: 100%"}},"d406a9bb0a7340b5bb5647727dde1688":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d50fa1618e9b41808e660e8ee1294be0":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd4fd245465b49efa0dbf2e311a87c29":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e254bb46d0e2488b9bc358fe2a2b40d6":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_66f7226a40b240b4a3641d635a2ef53f","IPY_MODEL_44e26082a459439ab15c52186951382e","IPY_MODEL_12a0a7c385a64c8088cece9de6ba35ec"],"layout":"IPY_MODEL_9349a071a39d4715b9f8872b38a83107"}},"e2bd641b7cf644c6add67d418862d101":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"e9937166be26402cae13c6f03391ab17":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"eb1426c802cf4d72abda9d5653c2b2f8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f10b7f60c3e94be5b0d77bf41380c333":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_59439f0d8ecb4d26bc39f0271a23a2b5","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_34efd389c8bd416c9dfeca26e3ee1acc","value":1355256}},"f1f76bda969748c29f84b07ffde60c4d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"f7cdcfe516694ebab924e025b77d608f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_847c9c8f934e40b7815444deca13984f","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f1f76bda969748c29f84b07ffde60c4d","value":1042301}},"fa45d12b6ab74135b97987b499dc8f52":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_9b2ba5ee692340a5b7abef958d93b663","placeholder":"​","style":"IPY_MODEL_adc807a87e4746459e6534b965c669c4","value":" 456k/456k [00:00&lt;00:00, 11.4MB/s]"}},"fae773309b3a4a98ab1444444bb2c4a5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10774999,"sourceType":"datasetVersion","datasetId":6685068},{"sourceId":261567,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":223659,"modelId":245408},{"sourceId":277304,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":237492,"modelId":259172}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install pennylane\n!pip install timm","metadata":{"id":"l9585dSlA0fb","outputId":"b972ff3b-8a20-42c0-d8ee-620a76016ac4","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:32:31.951906Z","iopub.execute_input":"2025-03-11T14:32:31.952233Z","iopub.status.idle":"2025-03-11T14:32:43.755654Z","shell.execute_reply.started":"2025-03-11T14:32:31.952206Z","shell.execute_reply":"2025-03-11T14:32:43.754811Z"}},"outputs":[{"name":"stdout","text":"Collecting pennylane\n  Downloading PennyLane-0.40.0-py3-none-any.whl.metadata (10 kB)\nRequirement already satisfied: numpy<2.1 in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.26.4)\nRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.13.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from pennylane) (3.4.2)\nCollecting rustworkx>=0.14.0 (from pennylane)\n  Downloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\nRequirement already satisfied: autograd in /usr/local/lib/python3.10/dist-packages (from pennylane) (1.7.0)\nCollecting tomlkit (from pennylane)\n  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\nCollecting appdirs (from pennylane)\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nCollecting autoray>=0.6.11 (from pennylane)\n  Downloading autoray-0.7.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from pennylane) (5.5.0)\nCollecting pennylane-lightning>=0.40 (from pennylane)\n  Downloading PennyLane_Lightning-0.40.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (27 kB)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pennylane) (2.32.3)\nRequirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from pennylane) (4.12.2)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from pennylane) (24.2)\nCollecting diastatic-malt (from pennylane)\n  Downloading diastatic_malt-2.15.2-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<2.1->pennylane) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<2.1->pennylane) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<2.1->pennylane) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<2.1->pennylane) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<2.1->pennylane) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<2.1->pennylane) (2.4.1)\nCollecting scipy-openblas32>=0.3.26 (from pennylane-lightning>=0.40->pennylane)\n  Downloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.1/56.1 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from diastatic-malt->pennylane) (1.6.3)\nRequirement already satisfied: gast in /usr/local/lib/python3.10/dist-packages (from diastatic-malt->pennylane) (0.6.0)\nRequirement already satisfied: termcolor in /usr/local/lib/python3.10/dist-packages (from diastatic-malt->pennylane) (2.5.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pennylane) (2025.1.31)\nRequirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->diastatic-malt->pennylane) (0.45.1)\nRequirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->diastatic-malt->pennylane) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.1->pennylane) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<2.1->pennylane) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<2.1->pennylane) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<2.1->pennylane) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<2.1->pennylane) (2024.2.0)\nDownloading PennyLane-0.40.0-py3-none-any.whl (2.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading autoray-0.7.0-py3-none-any.whl (930 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m930.0/930.0 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading PennyLane_Lightning-0.40.0-cp310-cp310-manylinux_2_28_x86_64.whl (2.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading rustworkx-0.16.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nDownloading diastatic_malt-2.15.2-py3-none-any.whl (167 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 kB\u001b[0m \u001b[31m11.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\nDownloading scipy_openblas32-0.3.29.0.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m81.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: appdirs, tomlkit, scipy-openblas32, autoray, diastatic-malt, rustworkx, pennylane-lightning, pennylane\nSuccessfully installed appdirs-1.4.4 autoray-0.7.0 diastatic-malt-2.15.2 pennylane-0.40.0 pennylane-lightning-0.40.0 rustworkx-0.16.0 scipy-openblas32-0.3.29.0.0 tomlkit-0.13.2\nRequirement already satisfied: timm in /usr/local/lib/python3.10/dist-packages (1.0.12)\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from timm) (2.5.1+cu121)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (from timm) (0.20.1+cu121)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from timm) (6.0.2)\nRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from timm) (0.29.0)\nRequirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from timm) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (3.17.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2024.12.0)\nRequirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (24.2)\nRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (2.32.3)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.67.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->timm) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->timm) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision->timm) (11.0.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->timm) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->torchvision->timm) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub->timm) (2025.1.31)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->torchvision->timm) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->torchvision->timm) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->torchvision->timm) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->torchvision->timm) (2024.2.0)\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"dZm1-TcdBAiJ","outputId":"b78c78d2-6499-40f4-dbc8-4a4b649bc238","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:17:38.620852Z","iopub.execute_input":"2025-03-11T14:17:38.621070Z","iopub.status.idle":"2025-03-11T14:17:38.624671Z","shell.execute_reply.started":"2025-03-11T14:17:38.621051Z","shell.execute_reply":"2025-03-11T14:17:38.623793Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"import tarfile\nimport os\nos.makedirs(\"/kaggle/working/xray_images\")\n# cwd = os.getcwd()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:32:49.835316Z","iopub.execute_input":"2025-03-11T14:32:49.835614Z","iopub.status.idle":"2025-03-11T14:32:49.842141Z","shell.execute_reply.started":"2025-03-11T14:32:49.835592Z","shell.execute_reply":"2025-03-11T14:32:49.841422Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"images = tarfile.open('/kaggle/input/dataset-chest-xrays-vit/NLMCXR_png.tgz')\nimages.extractall('/kaggle/working/xray_images/')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:32:51.578386Z","iopub.execute_input":"2025-03-11T14:32:51.578672Z","iopub.status.idle":"2025-03-11T14:33:13.269202Z","shell.execute_reply.started":"2025-03-11T14:32:51.578651Z","shell.execute_reply":"2025-03-11T14:33:13.268324Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv(\"/kaggle/input/dataset-chest-xrays-vit/reports.csv\")\ndf.head()","metadata":{"id":"lgZdpVpgBGEq","outputId":"f022e53a-99fd-4e5e-b6f6-26db9d8bd942","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:33:41.283054Z","iopub.execute_input":"2025-03-11T14:33:41.283379Z","iopub.status.idle":"2025-03-11T14:33:41.684976Z","shell.execute_reply.started":"2025-03-11T14:33:41.283353Z","shell.execute_reply":"2025-03-11T14:33:41.684023Z"}},"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"                 image_name             image_caption      comparison  \\\n0  CXR1524_IM-0339-1001.png  pa and lat view chest pm            none   \n1  CXR1524_IM-0339-2001.png  pa and lat view chest pm            none   \n2  CXR2026_IM-0671-1001.png  ray chest pa and lateral  none available   \n3  CXR2026_IM-0671-2001.png  ray chest pa and lateral  none available   \n4  CXR1441_IM-0285-1001.png  ray chest pa and lateral            none   \n\n                                indication  \\\n0  preop right knee total knee replacement   \n1  preop right knee total knee replacement   \n2          the patient is a with back pain   \n3          the patient is a with back pain   \n4         hypertension preop hiatal hernia   \n\n                                            findings  \\\n0  heart size is normal the aorta is tortuous and...   \n1  heart size is normal the aorta is tortuous and...   \n2  the trachea is midline cardiomediastinal silho...   \n3  the trachea is midline cardiomediastinal silho...   \n4  the heart is normal in size the mediastinum is...   \n\n                                          impression  image_count  \\\n0  no acute cardiopulmonary process tortuous aort...            2   \n1  no acute cardiopulmonary process tortuous aort...            2   \n2             no acute cardiopulmonary abnormalities            2   \n3             no acute cardiopulmonary abnormalities            2   \n4  no acute disease retrocardiac density correspo...            2   \n\n   indication_count  findings_count  impression_count  \n0                 6              34                11  \n1                 6              34                11  \n2                 7              32                 4  \n3                 7              32                 4  \n4                 4              33                10  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>image_caption</th>\n      <th>comparison</th>\n      <th>indication</th>\n      <th>findings</th>\n      <th>impression</th>\n      <th>image_count</th>\n      <th>indication_count</th>\n      <th>findings_count</th>\n      <th>impression_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CXR1524_IM-0339-1001.png</td>\n      <td>pa and lat view chest pm</td>\n      <td>none</td>\n      <td>preop right knee total knee replacement</td>\n      <td>heart size is normal the aorta is tortuous and...</td>\n      <td>no acute cardiopulmonary process tortuous aort...</td>\n      <td>2</td>\n      <td>6</td>\n      <td>34</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CXR1524_IM-0339-2001.png</td>\n      <td>pa and lat view chest pm</td>\n      <td>none</td>\n      <td>preop right knee total knee replacement</td>\n      <td>heart size is normal the aorta is tortuous and...</td>\n      <td>no acute cardiopulmonary process tortuous aort...</td>\n      <td>2</td>\n      <td>6</td>\n      <td>34</td>\n      <td>11</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CXR2026_IM-0671-1001.png</td>\n      <td>ray chest pa and lateral</td>\n      <td>none available</td>\n      <td>the patient is a with back pain</td>\n      <td>the trachea is midline cardiomediastinal silho...</td>\n      <td>no acute cardiopulmonary abnormalities</td>\n      <td>2</td>\n      <td>7</td>\n      <td>32</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CXR2026_IM-0671-2001.png</td>\n      <td>ray chest pa and lateral</td>\n      <td>none available</td>\n      <td>the patient is a with back pain</td>\n      <td>the trachea is midline cardiomediastinal silho...</td>\n      <td>no acute cardiopulmonary abnormalities</td>\n      <td>2</td>\n      <td>7</td>\n      <td>32</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CXR1441_IM-0285-1001.png</td>\n      <td>ray chest pa and lateral</td>\n      <td>none</td>\n      <td>hypertension preop hiatal hernia</td>\n      <td>the heart is normal in size the mediastinum is...</td>\n      <td>no acute disease retrocardiac density correspo...</td>\n      <td>2</td>\n      <td>4</td>\n      <td>33</td>\n      <td>10</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":4},{"cell_type":"code","source":"dataFrame = df[[\"image_name\",\"findings\"]]\ndataFrame.head()","metadata":{"id":"rn6WYbPwBQ_m","outputId":"47f2bdb6-1d70-4f8b-f89a-f8ea17e61303","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:33:42.842745Z","iopub.execute_input":"2025-03-11T14:33:42.843095Z","iopub.status.idle":"2025-03-11T14:33:42.873498Z","shell.execute_reply.started":"2025-03-11T14:33:42.843066Z","shell.execute_reply":"2025-03-11T14:33:42.872592Z"}},"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                 image_name                                           findings\n0  CXR1524_IM-0339-1001.png  heart size is normal the aorta is tortuous and...\n1  CXR1524_IM-0339-2001.png  heart size is normal the aorta is tortuous and...\n2  CXR2026_IM-0671-1001.png  the trachea is midline cardiomediastinal silho...\n3  CXR2026_IM-0671-2001.png  the trachea is midline cardiomediastinal silho...\n4  CXR1441_IM-0285-1001.png  the heart is normal in size the mediastinum is...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image_name</th>\n      <th>findings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CXR1524_IM-0339-1001.png</td>\n      <td>heart size is normal the aorta is tortuous and...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CXR1524_IM-0339-2001.png</td>\n      <td>heart size is normal the aorta is tortuous and...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CXR2026_IM-0671-1001.png</td>\n      <td>the trachea is midline cardiomediastinal silho...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CXR2026_IM-0671-2001.png</td>\n      <td>the trachea is midline cardiomediastinal silho...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CXR1441_IM-0285-1001.png</td>\n      <td>the heart is normal in size the mediastinum is...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"dataFrame.rename(columns = {'image_name':'image','findings':'caption'}, inplace = True)\ndataFrame.head()","metadata":{"id":"XcQ4RWkvBTfu","outputId":"0d29f25f-f354-4037-b9c4-fc0bf20ff8a5","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:33:43.855993Z","iopub.execute_input":"2025-03-11T14:33:43.856345Z","iopub.status.idle":"2025-03-11T14:33:43.865157Z","shell.execute_reply.started":"2025-03-11T14:33:43.856314Z","shell.execute_reply":"2025-03-11T14:33:43.864357Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-6-f09a539ac735>:1: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dataFrame.rename(columns = {'image_name':'image','findings':'caption'}, inplace = True)\n","output_type":"stream"},{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                      image                                            caption\n0  CXR1524_IM-0339-1001.png  heart size is normal the aorta is tortuous and...\n1  CXR1524_IM-0339-2001.png  heart size is normal the aorta is tortuous and...\n2  CXR2026_IM-0671-1001.png  the trachea is midline cardiomediastinal silho...\n3  CXR2026_IM-0671-2001.png  the trachea is midline cardiomediastinal silho...\n4  CXR1441_IM-0285-1001.png  the heart is normal in size the mediastinum is...","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>CXR1524_IM-0339-1001.png</td>\n      <td>heart size is normal the aorta is tortuous and...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>CXR1524_IM-0339-2001.png</td>\n      <td>heart size is normal the aorta is tortuous and...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>CXR2026_IM-0671-1001.png</td>\n      <td>the trachea is midline cardiomediastinal silho...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>CXR2026_IM-0671-2001.png</td>\n      <td>the trachea is midline cardiomediastinal silho...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>CXR1441_IM-0285-1001.png</td>\n      <td>the heart is normal in size the mediastinum is...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from pathlib import Path\nbase_path = Path('/kaggle/working/xray_images/')\ndataFrame['image'] = dataFrame['image'].apply(\n    lambda x: str(base_path / x )\n)\n","metadata":{"id":"g2PeCgxHBU8z","outputId":"ebe84aab-aa95-4e0e-bd5d-6b6724bc3865","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:33:44.830693Z","iopub.execute_input":"2025-03-11T14:33:44.831031Z","iopub.status.idle":"2025-03-11T14:33:44.865411Z","shell.execute_reply.started":"2025-03-11T14:33:44.831004Z","shell.execute_reply":"2025-03-11T14:33:44.864599Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-7-bbe893013d98>:3: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  dataFrame['image'] = dataFrame['image'].apply(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"dataFrame.head()","metadata":{"id":"lxsOlrZNLxaU","outputId":"3c8b283a-2efd-4105-e5ec-36ea5b8e88ec","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:33:45.916622Z","iopub.execute_input":"2025-03-11T14:33:45.917005Z","iopub.status.idle":"2025-03-11T14:33:45.924609Z","shell.execute_reply.started":"2025-03-11T14:33:45.916972Z","shell.execute_reply":"2025-03-11T14:33:45.924070Z"}},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"                                               image  \\\n0  /kaggle/working/xray_images/CXR1524_IM-0339-10...   \n1  /kaggle/working/xray_images/CXR1524_IM-0339-20...   \n2  /kaggle/working/xray_images/CXR2026_IM-0671-10...   \n3  /kaggle/working/xray_images/CXR2026_IM-0671-20...   \n4  /kaggle/working/xray_images/CXR1441_IM-0285-10...   \n\n                                             caption  \n0  heart size is normal the aorta is tortuous and...  \n1  heart size is normal the aorta is tortuous and...  \n2  the trachea is midline cardiomediastinal silho...  \n3  the trachea is midline cardiomediastinal silho...  \n4  the heart is normal in size the mediastinum is...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>caption</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>/kaggle/working/xray_images/CXR1524_IM-0339-10...</td>\n      <td>heart size is normal the aorta is tortuous and...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>/kaggle/working/xray_images/CXR1524_IM-0339-20...</td>\n      <td>heart size is normal the aorta is tortuous and...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>/kaggle/working/xray_images/CXR2026_IM-0671-10...</td>\n      <td>the trachea is midline cardiomediastinal silho...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>/kaggle/working/xray_images/CXR2026_IM-0671-20...</td>\n      <td>the trachea is midline cardiomediastinal silho...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>/kaggle/working/xray_images/CXR1441_IM-0285-10...</td>\n      <td>the heart is normal in size the mediastinum is...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport os\n\nfrom torch.optim import lr_scheduler\nimport torchvision\nfrom torchvision import datasets, transforms\nimport torchvision.models as models\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom timm import create_model, list_models\nfrom types import SimpleNamespace\nfrom transformers import GPT2LMHeadModel, GPT2TokenizerFast, get_linear_schedule_with_warmup\nfrom transformers import AutoTokenizer, AutoModelForMaskedLM\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom PIL import Image\nfrom pathlib import Path\nfrom sklearn.model_selection import train_test_split\nfrom torch.cuda.amp import GradScaler, autocast\nfrom tqdm.auto import tqdm\nimport gc\nimport json\nimport time\nimport copy\n\n# Pennylane\nimport pennylane as qml\nfrom pennylane import numpy as np\n\ntorch.manual_seed(42)\nnp.random.seed(42)\n\n# Plotting\nimport matplotlib.pyplot as plt\n\n# OpenMP: number of parallel threads.\nos.environ[\"OMP_NUM_THREADS\"] = \"16\"\n\nseconds = time.time()\nprint(\"Time in seconds since beginning of run:\", seconds)\nlocal_time = time.ctime(seconds)\nprint(local_time)","metadata":{"id":"equrqeKTBWnA","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:33:46.714128Z","iopub.execute_input":"2025-03-11T14:33:46.714420Z","iopub.status.idle":"2025-03-11T14:34:10.837381Z","shell.execute_reply.started":"2025-03-11T14:33:46.714398Z","shell.execute_reply":"2025-03-11T14:34:10.836477Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/albumentations/__init__.py:24: UserWarning: A new version of Albumentations is available: 2.0.5 (you have 1.4.20). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n  check_for_updates()\n","output_type":"stream"},{"name":"stdout","text":"Time in seconds since beginning of run: 1741703650.833553\nTue Mar 11 14:34:10 2025\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"%env TOKENIZERS_PARALLELISM = false","metadata":{"id":"ugkbBH-5BYoY","outputId":"712ce925-25ed-4d96-936b-7e6cd89d92fd","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:12.191520Z","iopub.execute_input":"2025-03-11T14:34:12.192248Z","iopub.status.idle":"2025-03-11T14:34:12.197443Z","shell.execute_reply.started":"2025-03-11T14:34:12.192203Z","shell.execute_reply":"2025-03-11T14:34:12.196569Z"}},"outputs":[{"name":"stdout","text":"env: TOKENIZERS_PARALLELISM=false\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"sample_tfms = [\n    A.HorizontalFlip(),\n    A.RandomBrightnessContrast(),\n    A.ColorJitter(),\n    A.ShiftScaleRotate(shift_limit=0.1, scale_limit=0.3, rotate_limit=45, p=0.5),\n    A.HueSaturationValue(p=0.3),\n]\ntrain_tfms = A.Compose([\n    *sample_tfms,\n    A.Resize(224,224),\n    A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n    ToTensorV2()\n])\nvalid_tfms = A.Compose([\n    A.Resize(224,224),\n    A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n    ToTensorV2()\n])","metadata":{"id":"WvWcw2TaBa7M","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:13.231775Z","iopub.execute_input":"2025-03-11T14:34:13.232132Z","iopub.status.idle":"2025-03-11T14:34:13.244981Z","shell.execute_reply.started":"2025-03-11T14:34:13.232106Z","shell.execute_reply":"2025-03-11T14:34:13.243880Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.pad_token","metadata":{"id":"q-wQPaBeBcla","outputId":"4322e779-bf9a-4dd4-aabd-24e90582dcaf","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:18.015278Z","iopub.execute_input":"2025-03-11T14:34:18.015703Z","iopub.status.idle":"2025-03-11T14:34:23.074112Z","shell.execute_reply.started":"2025-03-11T14:34:18.015667Z","shell.execute_reply":"2025-03-11T14:34:23.073295Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cecf40a3202c49688f72525b8b801ef2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1602dac2e6334a6091957ca6049fdbce"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29c9327e052a4a5d854adb43085472b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f200414f942943c4adecd3c99ea8f80f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2235629150a540daa4e38e4d4d00d9f2"}},"metadata":{}},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"'<|endoftext|>'"},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"train_df, val_df = train_test_split(dataFrame,test_size=0.1)\ntrain_df.reset_index(drop=True,inplace=True)\nval_df.reset_index(drop=True,inplace=True)\nprint(len(train_df),len(val_df))","metadata":{"id":"iqBz_tXhBfEo","outputId":"f71a7f4e-1351-4c87-8fb4-f58c9596a0a4","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:24.879897Z","iopub.execute_input":"2025-03-11T14:34:24.880310Z","iopub.status.idle":"2025-03-11T14:34:24.889780Z","shell.execute_reply.started":"2025-03-11T14:34:24.880272Z","shell.execute_reply":"2025-03-11T14:34:24.888831Z"}},"outputs":[{"name":"stdout","text":"6321 703\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"class Dataset:\n    def __init__(self, df, tfms):\n        self.df = df\n        self.tfms = tfms\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self,idx):\n        sample = self.df.iloc[idx,:]\n        image = sample['image']\n        caption = sample['caption']\n        image = Image.open(image).convert('RGB')\n        image = np.array(image)\n        augs = self.tfms(image=image)\n        image = augs['image']\n        caption = f\"{caption}<|endoftext|>\"\n        input_ids = tokenizer(\n            caption,\n            truncation=True)['input_ids']\n        labels = input_ids.copy()\n        labels[:-1] = input_ids[1:]\n        return image,input_ids,labels","metadata":{"id":"GKzWVXAMBj-x","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:26.401994Z","iopub.execute_input":"2025-03-11T14:34:26.402281Z","iopub.status.idle":"2025-03-11T14:34:26.407793Z","shell.execute_reply.started":"2025-03-11T14:34:26.402260Z","shell.execute_reply":"2025-03-11T14:34:26.406973Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"train_ds = Dataset(train_df,train_tfms)\nval_ds = Dataset(val_df,valid_tfms)","metadata":{"id":"3SG0X_Q0Blk_","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:27.732458Z","iopub.execute_input":"2025-03-11T14:34:27.732910Z","iopub.status.idle":"2025-03-11T14:34:27.737257Z","shell.execute_reply.started":"2025-03-11T14:34:27.732852Z","shell.execute_reply":"2025-03-11T14:34:27.736328Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"def collate_fn(batch):\n    image = [i[0] for i in batch]\n    input_ids = [i[1] for i in batch]\n    labels = [i[2] for i in batch]\n    image = torch.stack(image,dim=0)\n    input_ids = tokenizer.pad(\n        {'input_ids':input_ids},\n        padding='longest',\n        return_attention_mask=False,\n        return_tensors='pt'\n    )['input_ids']\n    labels = tokenizer.pad(\n        {'input_ids':labels},\n        padding='longest',\n        return_attention_mask=False,\n        return_tensors='pt'\n    )['input_ids']\n    mask = (input_ids!=tokenizer.pad_token_id).long()\n    labels[mask==0]=-100\n    return image, input_ids, labels\n","metadata":{"id":"nVrBpLaEBnaV","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:28.726492Z","iopub.execute_input":"2025-03-11T14:34:28.726935Z","iopub.status.idle":"2025-03-11T14:34:28.734767Z","shell.execute_reply.started":"2025-03-11T14:34:28.726900Z","shell.execute_reply":"2025-03-11T14:34:28.733683Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"dl = torch.utils.data.DataLoader(train_ds,shuffle=True,batch_size=2,collate_fn=collate_fn)\n# _,c,l = next(iter(dl))\n# print(c[0])\n# print(l[0])","metadata":{"id":"PWNiM-eEBpF7","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:29.676706Z","iopub.execute_input":"2025-03-11T14:34:29.677022Z","iopub.status.idle":"2025-03-11T14:34:29.681110Z","shell.execute_reply.started":"2025-03-11T14:34:29.676999Z","shell.execute_reply":"2025-03-11T14:34:29.679935Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"def H_layer(nqubits):\n    \"\"\"Layer of single-qubit Hadamard gates.\n    \"\"\"\n    for idx in range(nqubits):\n        qml.Hadamard(wires=idx)\n\n\ndef RY_layer(w):\n    \"\"\"Layer of parametrized qubit rotations around the y axis.\n    \"\"\"\n    for idx, element in enumerate(w):\n        qml.RY(element, wires=idx)\n\n\ndef entangling_layer(nqubits):\n    \"\"\"Layer of CNOTs followed by another shifted layer of CNOT.\n    \"\"\"\n    # In other words it should apply something like :\n    # CNOT  CNOT  CNOT  CNOT...  CNOT\n    #   CNOT  CNOT  CNOT...  CNOT\n    for i in range(0, nqubits - 1, 2):  # Loop over even indices: i=0,2,...N-2\n        qml.CNOT(wires=[i, i + 1])\n    for i in range(1, nqubits - 1, 2):  # Loop over odd indices:  i=1,3,...N-3\n        qml.CNOT(wires=[i, i + 1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:30.742133Z","iopub.execute_input":"2025-03-11T14:34:30.742417Z","iopub.status.idle":"2025-03-11T14:34:30.747876Z","shell.execute_reply.started":"2025-03-11T14:34:30.742395Z","shell.execute_reply":"2025-03-11T14:34:30.746912Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"n_qubits = 20               # Number of qubits\nstep = 0.0006               # Learning rate\nbatch_size = 17             # Number of samples for each training step\nnum_epochs = 30             # Number of training epochs\nq_depth = 6                 # Depth of the quantum circuit (number of variational layers)\ngamma_lr_scheduler = 0.1    # Learning rate reduction applied every 10 epochs.\nq_delta = 0.01              # Initial spread of random quantum weights\nstart_time = time.time() ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:32.160114Z","iopub.execute_input":"2025-03-11T14:34:32.160406Z","iopub.status.idle":"2025-03-11T14:34:32.164536Z","shell.execute_reply.started":"2025-03-11T14:34:32.160384Z","shell.execute_reply":"2025-03-11T14:34:32.163753Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"dev = qml.device(\"default.qubit\", wires=n_qubits)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:32.541616Z","iopub.execute_input":"2025-03-11T14:34:32.541947Z","iopub.status.idle":"2025-03-11T14:34:32.546744Z","shell.execute_reply.started":"2025-03-11T14:34:32.541921Z","shell.execute_reply":"2025-03-11T14:34:32.545884Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"@qml.qnode(dev, interface=\"torch\")\ndef quantum_net(q_input_features, q_weights_flat):\n    \"\"\"\n    The variational quantum circuit.\n    \"\"\"\n\n    # Reshape weights\n    q_weights = q_weights_flat.reshape(q_depth, n_qubits)\n\n    # Start from state |+> , unbiased w.r.t. |0> and |1>\n    H_layer(n_qubits)\n\n    # Embed features in the quantum node\n    #RY_layer(q_input_features)\n\n    # Expectation values in the Z basis\n    exp_vals = [qml.expval(qml.PauliZ(position)) for position in range(n_qubits)]\n    return tuple(exp_vals)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:33.426662Z","iopub.execute_input":"2025-03-11T14:34:33.426978Z","iopub.status.idle":"2025-03-11T14:34:33.431751Z","shell.execute_reply.started":"2025-03-11T14:34:33.426955Z","shell.execute_reply":"2025-03-11T14:34:33.431049Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class DressedQuantumNet(nn.Module):\n    \"\"\"\n    Torch module implementing the *dressed* quantum net.\n    \"\"\"\n\n    def __init__(self):\n        \"\"\"\n        Definition of the *dressed* layout.\n        \"\"\"\n\n        super().__init__()\n        self.pre_net = nn.Linear(768, n_qubits)  # Adjusted input dimension\n        self.q_params = nn.Parameter(q_delta * torch.randn(q_depth * n_qubits))\n        self.post_net = nn.Linear(n_qubits, 44)\n\n    def forward(self, input_features):\n        \"\"\"\n        Defining how tensors are supposed to move through the *dressed* quantum\n        net.\n        \"\"\"\n\n        # obtain the input features for the quantum circuit\n        # by reducing the feature dimension from 512 to 4\n        pre_out = self.pre_net(input_features)\n        q_in = torch.tanh(pre_out) * np.pi / 2.0\n\n        # Reshape the input for ViT\n        b, c, h, w = q_in.size()\n        q_in = q_in.view(b, c * h * w).unsqueeze(1)\n\n        # Apply the quantum circuit to each element of the batch and append to q_out\n        q_out = torch.Tensor(0, n_qubits)\n        q_out = q_out.to(device)\n        for elem in q_in:\n            q_out_elem = torch.hstack(quantum_net(elem, self.q_params)).float().unsqueeze(0)\n            q_out = torch.cat((q_out, q_out_elem))\n\n        # Flatten the quantum output\n        q_out = q_out.view(b, -1)\n\n        # return the two-dimensional prediction from the postprocessing layer\n        return self.post_net(q_out)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:34.385856Z","iopub.execute_input":"2025-03-11T14:34:34.386160Z","iopub.status.idle":"2025-03-11T14:34:34.392502Z","shell.execute_reply.started":"2025-03-11T14:34:34.386138Z","shell.execute_reply":"2025-03-11T14:34:34.391501Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"class GPT2Attention(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.n_heads = config.num_heads\n        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n        self.head_size = self.embed_dim // self.n_heads\n        self.seq_len = config.seq_len\n\n        self.c_attn = nn.Linear(self.embed_dim, self.head_size * self.n_heads * 3,bias=True)\n        self.scale = self.head_size ** -0.5\n\n        self.register_buffer('mask',torch.tril(torch.ones(1,1,self.seq_len,self.seq_len)))\n\n        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n\n        self.attn_dropout = nn.Dropout(config.attention_dropout)\n        self.resid_dropout = nn.Dropout(config.residual_dropout)\n\n\n    def forward(self, x):\n        b,t,c = x.shape\n        # q,k,v shape individually: batch_size x seq_len x embed_dim\n        # we know that qk_t = q x k_t, where q=bxtxhead_dim, k_t=bxhead_timxt\n        q,k,v = self.c_attn(x).chunk(3,dim=-1)\n        q = q.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n        k = k.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n        v = v.view(b,t,self.n_heads,self.head_size).permute(0,2,1,3)\n\n        qk_t = (q@k.transpose(-2,-1)) * self.scale\n        qk_t = qk_t.masked_fill(self.mask[:,:,:t,:t]==0,float('-inf'))\n        qk_t = F.softmax(qk_t,dim=-1)\n        weights = self.attn_dropout(qk_t)\n\n        attention = weights @ v # batch x n_heads x t x head_size\n        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n\n        out = self.c_proj(attention)\n        out = self.resid_dropout(out)\n\n        return out","metadata":{"id":"606GjEQlBr_n","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:35.646421Z","iopub.execute_input":"2025-03-11T14:34:35.646838Z","iopub.status.idle":"2025-03-11T14:34:35.658691Z","shell.execute_reply.started":"2025-03-11T14:34:35.646786Z","shell.execute_reply":"2025-03-11T14:34:35.657834Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"class GPT2CrossAttention(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.n_heads = config.num_heads\n        assert self.embed_dim % self.n_heads == 0, 'embedding dimension by be divisible by number of heads'\n        self.head_size = self.embed_dim // self.n_heads\n        self.seq_len = config.seq_len\n\n        self.q = nn.Linear(self.embed_dim,self.embed_dim)\n        self.k = nn.Linear(self.embed_dim,self.embed_dim)\n        self.v = nn.Linear(self.embed_dim,self.embed_dim)\n        self.scale = self.head_size ** -0.5\n\n        self.c_proj = nn.Linear(self.embed_dim, self.embed_dim, bias=True)\n\n        self.attn_dropout = nn.Dropout(config.attention_dropout)\n        self.resid_dropout = nn.Dropout(config.residual_dropout)\n\n        self.apply(self._init_weights)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n            if module.bias is not None:\n                torch.nn.init.zeros_(module.bias)\n\n\n    def forward(self, q,k,v):\n        b,t,c = q.shape\n\n        q = self.q(q)\n        k = self.k(k)\n        v = self.v(v)\n\n        q = q.view(b,q.size(1),self.n_heads,self.head_size).permute(0,2,1,3) # batch x n_heads x seq_len x head_dim\n        k = k.view(b,k.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n        v = v.view(b,v.size(1),self.n_heads,self.head_size).permute(0,2,1,3)\n\n        qk_t = (q@k.transpose(-2,-1)) * self.scale\n        qk_t = F.softmax(qk_t,dim=-1)\n        weights = self.attn_dropout(qk_t)\n\n        attention = weights @ v # batch x n_heads x t x head_size\n        attention = attention.permute(0,2,1,3).contiguous().view(b,t,c) # batch x t x embed_dim\n\n        out = self.c_proj(attention)\n        out = self.resid_dropout(out)\n\n        return out","metadata":{"id":"848GiucrBt0c","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:36.980972Z","iopub.execute_input":"2025-03-11T14:34:36.981266Z","iopub.status.idle":"2025-03-11T14:34:36.989687Z","shell.execute_reply.started":"2025-03-11T14:34:36.981244Z","shell.execute_reply":"2025-03-11T14:34:36.988760Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"class GPT2MLP(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.mlp_ratio = config.mlp_ratio\n        self.mlp_dropout = config.mlp_dropout\n\n        self.c_fc = nn.Linear(self.embed_dim,self.embed_dim*self.mlp_ratio)\n        self.c_proj = nn.Linear(self.embed_dim*self.mlp_ratio,self.embed_dim)\n        self.act = nn.GELU()\n        self.dropout = nn.Dropout(self.mlp_dropout)\n\n    def forward(self,x):\n        x = self.c_fc(x)\n        x = self.act(x)\n        x = self.c_proj(x)\n        x = self.dropout(x)\n        return x","metadata":{"id":"P-QgFkBMBwgH","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:38.642850Z","iopub.execute_input":"2025-03-11T14:34:38.643142Z","iopub.status.idle":"2025-03-11T14:34:38.648572Z","shell.execute_reply.started":"2025-03-11T14:34:38.643121Z","shell.execute_reply":"2025-03-11T14:34:38.647654Z"}},"outputs":[],"execution_count":25},{"cell_type":"code","source":"class GPT2Block(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n        self.embed_dim = config.embed_dim\n        self.ln_1 = nn.LayerNorm(self.embed_dim)\n        self.attn = GPT2Attention(config)\n        self.ln_2 = nn.LayerNorm(self.embed_dim)\n        self.mlp = GPT2MLP(config)\n        self.ln_3 = nn.LayerNorm(self.embed_dim)\n        self.cross_attn = GPT2CrossAttention(config)\n\n    def forward(self,x,enc_out):\n        x = x+self.attn(self.ln_1(x)) # Attention\n        x = x+self.cross_attn(self.ln_2(x),enc_out,enc_out) # Cross Attention\n        x = x+self.mlp(self.ln_3(x))\n        return x","metadata":{"id":"Eh7RVG5KBvey","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:38.971905Z","iopub.execute_input":"2025-03-11T14:34:38.972199Z","iopub.status.idle":"2025-03-11T14:34:38.977565Z","shell.execute_reply.started":"2025-03-11T14:34:38.972167Z","shell.execute_reply":"2025-03-11T14:34:38.976841Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"class VisionGPT2Model(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n\n        self.config = config\n\n        model_path = \"/kaggle/input/hybridmodelsqtl/pytorch/default/1/ViTQTLhybrid.pth\"    # Change this to your actual path\n        vit = torch.load(model_path, map_location=torch.device('cpu'))\n        print(vit)\n        self.patch_embed = vit.patch_embed\n        num_patches = self.patch_embed.num_patches # Number of patches\n\n        self.cls_token = vit.cls_token # class token\n        embed_len = num_patches + vit.num_prefix_tokens\n        self.pos_embed = vit.pos_embed\n        self.pos_drop = nn.Dropout(p=0.)\n\n        self.blocks = nn.ModuleList([vit.blocks[i] for i in range(config.depth)]) # Vision Transformer blocks\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size,config.embed_dim),\n            wpe = nn.Embedding(config.seq_len,config.embed_dim),\n            drop = nn.Dropout(config.emb_dropout),\n            h = nn.ModuleList([GPT2Block(config) for _ in range(config.depth)]),\n            ln_f = nn.LayerNorm(config.embed_dim)\n        ))\n        self.lm_head = nn.Linear(config.embed_dim,config.vocab_size,bias=False) # language model head, which maps the model's embeddings to the vocabulary size\n        self.transformer.wte.weight = self.lm_head.weight\n\n    def _pos_embed(self,x):\n        pos_embed = self.pos_embed\n        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n        x = x[:, :-1, :]\n        x = x + pos_embed\n        return self.pos_drop(x)\n\n    def pretrained_layers_trainable(self,trainable=False):\n        layers = [\n            self.cls_token, self.patch_embed, self.pos_embed, self.blocks,\n            self.transformer.wte, self.transformer.wpe,\n            self.transformer.ln_f, self.lm_head\n        ]\n        gpt_layers = [[\n            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn,self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        for l in gpt_layers:\n            layers.extend(l)\n\n        for layer in layers:\n            if not isinstance(layer,nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = trainable\n            else:\n                layer.requires_grad = trainable\n\n        total_frozen_params = sum([p.numel() for p in self.parameters() if not p.requires_grad])\n        print(f'{total_frozen_params=}')\n\n    def unfreeze_gpt_layers(self,):\n        gpt_layers = [[\n            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn,self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        flatten = []\n        for l in gpt_layers:\n            flatten.extend(l)\n\n        for layer in flatten:\n            if not isinstance(layer,nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = True\n            else:\n                layer.requires_grad = True\n\n    @classmethod\n    def from_pretrained(self,config):\n        model = VisionGPT2Model(config)\n        sd = model.state_dict()\n        keys = sd.keys()\n        ignore_matches = ['blocks.','cross_attn.','ln_3','cls_token','pos_embed','patch_embed.','.attn.mask']\n        vit_keys = [key for key in keys if any(match in key for match in ignore_matches)]\n        gpt_keys = [key for key in keys if key not in vit_keys]\n\n        gpt2_small = GPT2LMHeadModel.from_pretrained('gpt2')\n\n        sd_hf = gpt2_small.state_dict()\n        hf_keys = sd_hf.keys()\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.masked_bias')]\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.bias')]\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight',\"embeddings.word_embeddings.weight\"]\n\n        for k in hf_keys:\n            if any(match in k for match in ignore_matches):\n                continue\n            try:\n              if any(k.endswith(w) for w in transposed):\n                  # assert sd_hf[k].shape[::-1] == sd[k].shape\n                  with torch.no_grad():\n                      sd[k].copy_(sd_hf[k].t())\n              else:\n                  # assert sd_hf[k].shape == sd[k].shape\n                  with torch.no_grad():\n                      sd[k].copy_(sd_hf[k])\n            except:\n              pass\n\n        model.load_state_dict(sd)\n\n        return model\n\n    def forward(self,image,input_ids,labels=None):\n\n        image = self.patch_embed(image)\n        image = self._pos_embed(image)\n\n        token_embeddings = self.transformer.wte(input_ids) # batch x seq_len\n        pos_embs = torch.arange(0,input_ids.size(1)).to(input_ids.device)\n        positional_embeddings = self.transformer.wpe(pos_embs)\n        input_ids = self.transformer.drop(token_embeddings+positional_embeddings)\n\n        for i in range(self.config.depth):\n            image = self.blocks[i](image)\n            input_ids = self.transformer.h[i](input_ids, image)\n\n        input_ids = self.transformer.ln_f(input_ids)\n\n        if labels is not None:\n            lm_logits = self.lm_head(input_ids)\n            loss = F.cross_entropy(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))\n            return loss\n\n        lm_logits = self.lm_head(input_ids[:,[-1],:])\n        return lm_logits\n\n    def generate(self,image,sequence,max_tokens=50,temperature=1.0,deterministic=False):\n        for _ in range(max_tokens):\n            out = self(image,sequence)\n            out = out[:,-1,:] / temperature\n            probs = F.softmax(out,dim=-1)\n            if deterministic:\n                next_token = torch.argmax(probs,dim=-1,keepdim=True)\n            else:\n                next_token = torch.multinomial(probs,num_samples=1)\n            sequence = torch.cat([sequence,next_token],dim=1)\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n\n        return sequence.cpu().flatten()","metadata":{"id":"tunBu1OFBy2A","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:40.039468Z","iopub.execute_input":"2025-03-11T14:34:40.039762Z","iopub.status.idle":"2025-03-11T14:34:40.058278Z","shell.execute_reply.started":"2025-03-11T14:34:40.039741Z","shell.execute_reply":"2025-03-11T14:34:40.057373Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"class DEiTGPT2Model(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n\n        self.config = config\n\n        model_path = \"/kaggle/input/hybridmodelsqtl/pytorch/default/1/BEiT_DEiT_QTLhybrid.pth\"    # Change this to your actual path\n        vit = torch.load(model_path, map_location=torch.device('cpu'))\n        print(vit)\n        self.patch_embed = vit.patch_embed\n        num_patches = self.patch_embed.num_patches # Number of patches\n\n        self.cls_token = vit.cls_token # class token\n        embed_len = num_patches + vit.num_prefix_tokens\n        self.pos_embed = vit.pos_embed\n        self.pos_drop = nn.Dropout(p=0.)\n\n        self.blocks = nn.ModuleList([vit.blocks[i] for i in range(config.depth)]) # Vision Transformer blocks\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size,config.embed_dim),\n            wpe = nn.Embedding(config.seq_len,config.embed_dim),\n            drop = nn.Dropout(config.emb_dropout),\n            h = nn.ModuleList([GPT2Block(config) for _ in range(config.depth)]),\n            ln_f = nn.LayerNorm(config.embed_dim)\n        ))\n        self.lm_head = nn.Linear(config.embed_dim,config.vocab_size,bias=False) # language model head, which maps the model's embeddings to the vocabulary size\n        self.transformer.wte.weight = self.lm_head.weight\n\n    def _pos_embed(self,x):\n        pos_embed = self.pos_embed\n        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n        x = x + pos_embed\n        return self.pos_drop(x)\n\n    def pretrained_layers_trainable(self,trainable=False):\n        layers = [\n            self.cls_token, self.patch_embed, self.pos_embed, self.blocks,\n            self.transformer.wte, self.transformer.wpe,\n            self.transformer.ln_f, self.lm_head\n        ]\n        gpt_layers = [[\n            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn,self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        for l in gpt_layers:\n            layers.extend(l)\n\n        for layer in layers:\n            if not isinstance(layer,nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = trainable\n            else:\n                layer.requires_grad = trainable\n\n        total_frozen_params = sum([p.numel() for p in self.parameters() if not p.requires_grad])\n        print(f'{total_frozen_params=}')\n\n    def unfreeze_gpt_layers(self,):\n        gpt_layers = [[\n            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn,self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        flatten = []\n        for l in gpt_layers:\n            flatten.extend(l)\n\n        for layer in flatten:\n            if not isinstance(layer,nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = True\n            else:\n                layer.requires_grad = True\n\n    @classmethod\n    def from_pretrained(self,config):\n        model = DEiTGPT2Model(config)\n        sd = model.state_dict()\n        keys = sd.keys()\n        ignore_matches = ['blocks.','cross_attn.','ln_3','cls_token','pos_embed','patch_embed.','.attn.mask']\n        vit_keys = [key for key in keys if any(match in key for match in ignore_matches)]\n        gpt_keys = [key for key in keys if key not in vit_keys]\n\n        gpt2_small = GPT2LMHeadModel.from_pretrained('gpt2')\n\n        sd_hf = gpt2_small.state_dict()\n        hf_keys = sd_hf.keys()\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.masked_bias')]\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.bias')]\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight',\"embeddings.word_embeddings.weight\"]\n\n        for k in hf_keys:\n            if any(match in k for match in ignore_matches):\n                continue\n            try:\n              if any(k.endswith(w) for w in transposed):\n                  # assert sd_hf[k].shape[::-1] == sd[k].shape\n                  with torch.no_grad():\n                      sd[k].copy_(sd_hf[k].t())\n              else:\n                  # assert sd_hf[k].shape == sd[k].shape\n                  with torch.no_grad():\n                      sd[k].copy_(sd_hf[k])\n            except:\n              pass\n\n        model.load_state_dict(sd)\n\n        return model\n\n    def forward(self,image,input_ids,labels=None):\n\n        image = self.patch_embed(image)\n        image = self._pos_embed(image)\n\n        token_embeddings = self.transformer.wte(input_ids) # batch x seq_len\n        pos_embs = torch.arange(0,input_ids.size(1)).to(input_ids.device)\n        positional_embeddings = self.transformer.wpe(pos_embs)\n        input_ids = self.transformer.drop(token_embeddings+positional_embeddings)\n\n        for i in range(self.config.depth):\n            image = self.blocks[i](image)\n            input_ids = self.transformer.h[i](input_ids, image)\n\n        input_ids = self.transformer.ln_f(input_ids)\n\n        if labels is not None:\n            lm_logits = self.lm_head(input_ids)\n            loss = F.cross_entropy(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))\n            return loss\n\n        lm_logits = self.lm_head(input_ids[:,[-1],:])\n        return lm_logits\n\n    def generate(self,image,sequence,max_tokens=50,temperature=1.0,deterministic=False):\n        for _ in range(max_tokens):\n            out = self(image,sequence)\n            out = out[:,-1,:] / temperature\n            probs = F.softmax(out,dim=-1)\n            if deterministic:\n                next_token = torch.argmax(probs,dim=-1,keepdim=True)\n            else:\n                next_token = torch.multinomial(probs,num_samples=1)\n            sequence = torch.cat([sequence,next_token],dim=1)\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n\n        return sequence.cpu().flatten()","metadata":{"id":"xxWuQ_QNKbvX","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:44.078458Z","iopub.execute_input":"2025-03-11T14:34:44.078993Z","iopub.status.idle":"2025-03-11T14:34:44.102240Z","shell.execute_reply.started":"2025-03-11T14:34:44.078952Z","shell.execute_reply":"2025-03-11T14:34:44.101168Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"class BEiTGPT2Model(nn.Module):\n    def __init__(self,config):\n        super().__init__()\n\n        self.config = config\n\n        model_path = \"/kaggle/input/hybridmodelsqtl/pytorch/default/1/BEiT_DEiT_QTLhybrid.pth\"    # Change this to your actual path\n        vit = torch.load(model_path, map_location=torch.device('cpu'))\n        print(vit)\n        self.patch_embed = vit.patch_embed\n        num_patches = self.patch_embed.num_patches # Number of patches\n\n        self.cls_token = vit.cls_token # class token\n        embed_len = num_patches\n        self.pos_embed = vit.pos_embed\n        self.pos_drop = nn.Dropout(p=0.)\n\n        self.blocks = nn.ModuleList([vit.blocks[i] for i in range(config.depth)]) # Vision Transformer blocks\n\n        self.transformer = nn.ModuleDict(dict(\n            wte = nn.Embedding(config.vocab_size,config.embed_dim),\n            wpe = nn.Embedding(config.seq_len,config.embed_dim),\n            drop = nn.Dropout(config.emb_dropout),\n            h = nn.ModuleList([GPT2Block(config) for _ in range(config.depth)]),\n            ln_f = nn.LayerNorm(config.embed_dim)\n        ))\n        self.lm_head = nn.Linear(config.embed_dim,config.vocab_size,bias=False) # language model head, which maps the model's embeddings to the vocabulary size\n        self.transformer.wte.weight = self.lm_head.weight\n\n    def _pos_embed(self,x):\n        pos_embed = self.pos_embed\n        x = torch.cat((self.cls_token.expand(x.shape[0], -1, -1), x), dim=1)\n        # x = x[:, :-1, :]\n        x = x + pos_embed\n        return self.pos_drop(x)\n\n    def pretrained_layers_trainable(self,trainable=False):\n        layers = [\n            self.cls_token, self.patch_embed, self.pos_embed, self.blocks,\n            self.transformer.wte, self.transformer.wpe,\n            self.transformer.ln_f, self.lm_head\n        ]\n        gpt_layers = [[\n            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn,self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        for l in gpt_layers:\n            layers.extend(l)\n\n        for layer in layers:\n            if not isinstance(layer,nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = trainable\n            else:\n                layer.requires_grad = trainable\n\n        total_frozen_params = sum([p.numel() for p in self.parameters() if not p.requires_grad])\n        print(f'{total_frozen_params=}')\n\n    def unfreeze_gpt_layers(self,):\n        gpt_layers = [[\n            self.transformer.h[i].ln_1,self.transformer.h[i].ln_2,\n            self.transformer.h[i].attn,self.transformer.h[i].mlp\n        ] for i in range(self.config.depth)]\n        flatten = []\n        for l in gpt_layers:\n            flatten.extend(l)\n\n        for layer in flatten:\n            if not isinstance(layer,nn.Parameter):\n                for p in layer.parameters():\n                    p.requires_grad = True\n            else:\n                layer.requires_grad = True\n\n    @classmethod\n    def from_pretrained(self,config):\n        model = BEiTGPT2Model(config)\n        sd = model.state_dict()\n        keys = sd.keys()\n        ignore_matches = ['blocks.','cross_attn.','ln_3','cls_token','pos_embed','patch_embed.','.attn.mask']\n        vit_keys = [key for key in keys if any(match in key for match in ignore_matches)]\n        gpt_keys = [key for key in keys if key not in vit_keys]\n\n        gpt2_small = GPT2LMHeadModel.from_pretrained('gpt2')\n\n        sd_hf = gpt2_small.state_dict()\n        hf_keys = sd_hf.keys()\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.masked_bias')]\n        hf_keys = [k for k in hf_keys if not k.endswith('.attn.bias')]\n        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight',\"embeddings.word_embeddings.weight\"]\n\n        for k in hf_keys:\n            if any(match in k for match in ignore_matches):\n                continue\n            try:\n              if any(k.endswith(w) for w in transposed):\n                  # assert sd_hf[k].shape[::-1] == sd[k].shape\n                  with torch.no_grad():\n                      sd[k].copy_(sd_hf[k].t())\n              else:\n                  # assert sd_hf[k].shape == sd[k].shape\n                  with torch.no_grad():\n                      sd[k].copy_(sd_hf[k])\n            except:\n              pass\n\n        model.load_state_dict(sd)\n\n        return model\n\n    def forward(self,image,input_ids,labels=None):\n\n        image = self.patch_embed(image)\n        image = self._pos_embed(image)\n\n        token_embeddings = self.transformer.wte(input_ids) # batch x seq_len\n        pos_embs = torch.arange(0,input_ids.size(1)).to(input_ids.device)\n        positional_embeddings = self.transformer.wpe(pos_embs)\n        input_ids = self.transformer.drop(token_embeddings+positional_embeddings)\n\n        for i in range(self.config.depth):\n            image = self.blocks[i](image)\n            input_ids = self.transformer.h[i](input_ids, image)\n\n        input_ids = self.transformer.ln_f(input_ids)\n\n        if labels is not None:\n            lm_logits = self.lm_head(input_ids)\n            loss = F.cross_entropy(lm_logits.view(-1, lm_logits.shape[-1]), labels.view(-1))\n            return loss\n\n        lm_logits = self.lm_head(input_ids[:,[-1],:])\n        return lm_logits\n\n    def generate(self,image,sequence,max_tokens=50,temperature=1.0,deterministic=False):\n        for _ in range(max_tokens):\n            out = self(image,sequence)\n            out = out[:,-1,:] / temperature\n            probs = F.softmax(out,dim=-1)\n            if deterministic:\n                next_token = torch.argmax(probs,dim=-1,keepdim=True)\n            else:\n                next_token = torch.multinomial(probs,num_samples=1)\n            sequence = torch.cat([sequence,next_token],dim=1)\n            if next_token.item() == tokenizer.eos_token_id:\n                break\n\n        return sequence.cpu().flatten()","metadata":{"id":"8Sp0LrXmKb3v","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:47.039523Z","iopub.execute_input":"2025-03-11T14:34:47.039828Z","iopub.status.idle":"2025-03-11T14:34:47.056723Z","shell.execute_reply.started":"2025-03-11T14:34:47.039787Z","shell.execute_reply":"2025-03-11T14:34:47.055973Z"}},"outputs":[],"execution_count":29},{"cell_type":"code","source":"model_config = SimpleNamespace(\n    vocab_size = 50_257,\n    embed_dim = 768, # 768\n    num_heads = 12,\n    seq_len = 1024,\n    depth = 12,\n    attention_dropout = 0.1,\n    residual_dropout = 0.1,\n    mlp_ratio = 4,\n    mlp_dropout = 0.1,\n    emb_dropout = 0.1,\n)\ntrain_config = SimpleNamespace(\n    epochs = 20,\n    freeze_epochs_gpt = 1,\n    freeze_epochs_all = 2,\n    lr = 1e-4,\n    device = 'cuda',\n    model_path = Path('/kaggle/working/'),\n    batch_size = 32\n)\n","metadata":{"id":"gNsfojGEB1QU","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:34:49.920774Z","iopub.execute_input":"2025-03-11T14:34:49.921096Z","iopub.status.idle":"2025-03-11T14:34:49.925623Z","shell.execute_reply.started":"2025-03-11T14:34:49.921071Z","shell.execute_reply":"2025-03-11T14:34:49.924675Z"}},"outputs":[],"execution_count":30},{"cell_type":"code","source":"class TrainerViT:\n    def __init__(self,model_config,train_config, dls):\n\n        self.train_config = train_config\n        self.model_config = model_config\n        self.device = self.train_config.device\n\n        self.model = VisionGPT2Model.from_pretrained(model_config).to(self.device)\n        self.model.pretrained_layers_trainable(trainable=False)\n\n        print(f'trainable parameters: {sum([p.numel() for p in self.model.parameters() if p.requires_grad])}')\n\n        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        self.scaler = GradScaler()\n\n        self.train_dl, self.val_dl = dls\n\n        total_steps = len(self.train_dl)\n\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.train_config.lr / 25.)\n        self.sched = torch.optim.lr_scheduler.OneCycleLR(\n            self.optim,\n            max_lr=self.train_config.lr,\n            epochs=self.train_config.epochs,\n            steps_per_epoch=total_steps\n        )\n\n#         self.sched = get_linear_schedule_with_warmup(self.optim,num_warmup_steps=0,num_training_steps=total_steps)\n\n        self.metrics = pd.DataFrame()\n        self.metrics[['train_loss','train_perplexity','val_loss','val_perplexity']] = None\n\n        self.gen_tfms = A.Compose([\n            A.Resize(224,224),\n            A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n            ToTensorV2()\n        ])\n\n\n    def save_model(self,):\n        self.train_config.model_path.mkdir(exist_ok=True)\n        sd = self.model.state_dict()\n        torch.save(self.model,self.train_config.model_path/'captioner.pt')\n\n\n    def load_best_model(self,):\n        sd = torch.load(self.train_config.model_path/'captioner.pt')\n        self.model.load_state_dict(sd)\n\n\n    def train_one_epoch(self,epoch):\n\n        prog = tqdm(self.train_dl,total=len(self.train_dl))\n\n        running_loss = 0.\n\n        for image, input_ids, labels in prog:\n\n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n\n                loss = self.model(image,input_ids,labels)\n\n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optim)\n                self.scaler.update()\n                self.sched.step()\n                self.optim.zero_grad(set_to_none=True)\n\n                running_loss += loss.item()\n\n                prog.set_description(f'train loss: {loss.item():.3f}')\n\n            del image, input_ids, labels, loss\n\n        train_loss = running_loss / len(self.train_dl)\n        train_pxp = np.exp(train_loss)\n\n        self.metrics.loc[epoch,['train_loss','train_perplexity']] = (train_loss,train_pxp)\n\n\n    @torch.no_grad()\n    def valid_one_epoch(self,epoch):\n\n        prog = tqdm(self.val_dl,total=len(self.val_dl))\n\n        running_loss = 0.\n\n        for image, input_ids, labels in prog:\n\n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n\n                loss = self.model(image,input_ids,labels)\n                running_loss += loss.item()\n\n                prog.set_description(f'valid loss: {loss.item():.3f}')\n\n            del image, input_ids, labels, loss\n\n        val_loss = running_loss / len(self.val_dl)\n        val_pxp = np.exp(val_loss)\n\n        self.metrics.loc[epoch,['val_loss','val_perplexity']] = (val_loss,val_pxp)\n\n        return val_pxp\n\n\n    def clean(self):\n        gc.collect()\n        torch.cuda.empty_cache()\n\n\n    def fit(self,):\n\n        best_pxp = 1e9\n        best_epoch = -1\n        prog = tqdm(range(self.train_config.epochs))\n\n        for epoch in prog:\n\n            if epoch == self.train_config.freeze_epochs_gpt:\n                self.model.unfreeze_gpt_layers()\n                print('unfreezing GPT2 entirely...')\n\n            if epoch == self.train_config.freeze_epochs_all:\n                self.model.pretrained_layers_trainable(trainable=True)\n\n            self.model.train()\n            prog.set_description('training')\n            self.train_one_epoch(epoch)\n            self.clean()\n\n            self.model.eval()\n            prog.set_description('validating')\n            pxp = self.valid_one_epoch(epoch)\n            self.clean()\n\n            print(self.metrics.tail(1))\n\n            if pxp < best_pxp:\n                best_pxp = pxp\n                best_epoch = epoch\n                print('saving best model...')\n                self.save_model()\n\n        return {\n            'best_perplexity': best_pxp,\n            'best_epoch': best_epoch\n        }\n\n\n    @torch.no_grad()\n    def generate_caption(self,image,max_tokens=50,temperature=1.0,deterministic=False):\n\n        self.model.eval()\n\n        image = Image.open(image).convert('RGB')\n        image = np.array(image)\n        image = self.gen_tfms(image=image)['image']\n        image = image.unsqueeze(0).to(self.device)\n        sequence = torch.ones(1,1).to(device=self.device).long() * self.tokenizer.bos_token_id\n\n        caption = self.model.generate(\n            image,\n            sequence,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            deterministic=deterministic\n        )\n        caption = self.tokenizer.decode(caption.numpy(),skip_special_tokens=True)\n\n        return caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:35:01.676639Z","iopub.execute_input":"2025-03-11T14:35:01.677047Z","iopub.status.idle":"2025-03-11T14:35:01.692539Z","shell.execute_reply.started":"2025-03-11T14:35:01.677015Z","shell.execute_reply":"2025-03-11T14:35:01.691668Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"class TrainerDEiT:\n    def __init__(self,model_config,train_config, dls):\n\n        self.train_config = train_config\n        self.model_config = model_config\n        self.device = self.train_config.device\n\n        self.model = DEiTGPT2Model.from_pretrained(model_config).to(self.device)\n        self.model.pretrained_layers_trainable(trainable=False)\n\n        print(f'trainable parameters: {sum([p.numel() for p in self.model.parameters() if p.requires_grad])}')\n\n        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        self.scaler = GradScaler()\n\n        self.train_dl, self.val_dl = dls\n\n        total_steps = len(self.train_dl)\n\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.train_config.lr / 25.)\n        self.sched = torch.optim.lr_scheduler.OneCycleLR(\n            self.optim,\n            max_lr=self.train_config.lr,\n            epochs=self.train_config.epochs,\n            steps_per_epoch=total_steps\n        )\n\n#         self.sched = get_linear_schedule_with_warmup(self.optim,num_warmup_steps=0,num_training_steps=total_steps)\n\n        self.metrics = pd.DataFrame()\n        self.metrics[['train_loss','train_perplexity','val_loss','val_perplexity']] = None\n\n        self.gen_tfms = A.Compose([\n            A.Resize(224,224),\n            A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n            ToTensorV2()\n        ])\n\n\n    def save_model(self,):\n        self.train_config.model_path.mkdir(exist_ok=True)\n        sd = self.model.state_dict()\n        torch.save(self.model,self.train_config.model_path/'DEiTcaptioner_new.pt')\n\n\n    def load_best_model(self,):\n        sd = torch.load(self.train_config.model_path/'DEiTcaptioner_new.pt')\n        self.model.load_state_dict(sd)\n\n\n    def train_one_epoch(self,epoch):\n\n        prog = tqdm(self.train_dl,total=len(self.train_dl))\n\n        running_loss = 0.\n\n        for image, input_ids, labels in prog:\n\n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n\n                loss = self.model(image,input_ids,labels)\n\n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optim)\n                self.scaler.update()\n                self.sched.step()\n                self.optim.zero_grad(set_to_none=True)\n\n                running_loss += loss.item()\n\n                prog.set_description(f'train loss: {loss.item():.3f}')\n\n            del image, input_ids, labels, loss\n\n        train_loss = running_loss / len(self.train_dl)\n        train_pxp = np.exp(train_loss)\n\n        self.metrics.loc[epoch,['train_loss','train_perplexity']] = (train_loss,train_pxp)\n\n\n    @torch.no_grad()\n    def valid_one_epoch(self,epoch):\n\n        prog = tqdm(self.val_dl,total=len(self.val_dl))\n\n        running_loss = 0.\n\n        for image, input_ids, labels in prog:\n\n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n\n                loss = self.model(image,input_ids,labels)\n                running_loss += loss.item()\n\n                prog.set_description(f'valid loss: {loss.item():.3f}')\n\n            del image, input_ids, labels, loss\n\n        val_loss = running_loss / len(self.val_dl)\n        val_pxp = np.exp(val_loss)\n\n        self.metrics.loc[epoch,['val_loss','val_perplexity']] = (val_loss,val_pxp)\n\n        return val_pxp\n\n\n    def clean(self):\n        gc.collect()\n        torch.cuda.empty_cache()\n\n\n    def fit(self,):\n\n        best_pxp = 1e9\n        best_epoch = -1\n        prog = tqdm(range(self.train_config.epochs))\n\n        for epoch in prog:\n\n            if epoch == self.train_config.freeze_epochs_gpt:\n                self.model.unfreeze_gpt_layers()\n                print('unfreezing GPT2 entirely...')\n\n            if epoch == self.train_config.freeze_epochs_all:\n                self.model.pretrained_layers_trainable(trainable=True)\n\n            self.model.train()\n            prog.set_description('training')\n            self.train_one_epoch(epoch)\n            self.clean()\n\n            self.model.eval()\n            prog.set_description('validating')\n            pxp = self.valid_one_epoch(epoch)\n            self.clean()\n\n            print(self.metrics.tail(1))\n\n            if pxp < best_pxp:\n                best_pxp = pxp\n                best_epoch = epoch\n                print('saving best model...')\n                self.save_model()\n\n        return {\n            'best_perplexity': best_pxp,\n            'best_epoch': best_epoch\n        }\n\n\n    @torch.no_grad()\n    def generate_caption(self,image,max_tokens=50,temperature=1.0,deterministic=False):\n\n        self.model.eval()\n\n        image = Image.open(image).convert('RGB')\n        image = np.array(image)\n        image = self.gen_tfms(image=image)['image']\n        image = image.unsqueeze(0).to(self.device)\n        sequence = torch.ones(1,1).to(device=self.device).long() * self.tokenizer.bos_token_id\n\n        caption = self.model.generate(\n            image,\n            sequence,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            deterministic=deterministic\n        )\n        caption = self.tokenizer.decode(caption.numpy(),skip_special_tokens=True)\n\n        return caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:35:04.841165Z","iopub.execute_input":"2025-03-11T14:35:04.841474Z","iopub.status.idle":"2025-03-11T14:35:04.857127Z","shell.execute_reply.started":"2025-03-11T14:35:04.841452Z","shell.execute_reply":"2025-03-11T14:35:04.856256Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"class TrainerBEiT:\n    def __init__(self,model_config,train_config, dls):\n\n        self.train_config = train_config\n        self.model_config = model_config\n        self.device = self.train_config.device\n\n        self.model = BEiTGPT2Model.from_pretrained(model_config).to(self.device)\n        self.model.pretrained_layers_trainable(trainable=False)\n\n        print(f'trainable parameters: {sum([p.numel() for p in self.model.parameters() if p.requires_grad])}')\n\n        self.tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n        self.tokenizer.pad_token = self.tokenizer.eos_token\n\n        self.scaler = GradScaler()\n\n        self.train_dl, self.val_dl = dls\n\n        total_steps = len(self.train_dl)\n\n        self.optim = torch.optim.Adam(self.model.parameters(), lr=self.train_config.lr / 25.)\n        self.sched = torch.optim.lr_scheduler.OneCycleLR(\n            self.optim,\n            max_lr=self.train_config.lr,\n            epochs=self.train_config.epochs,\n            steps_per_epoch=total_steps\n        )\n\n#         self.sched = get_linear_schedule_with_warmup(self.optim,num_warmup_steps=0,num_training_steps=total_steps)\n\n        self.metrics = pd.DataFrame()\n        self.metrics[['train_loss','train_perplexity','val_loss','val_perplexity']] = None\n\n        self.gen_tfms = A.Compose([\n            A.Resize(224,224),\n            A.Normalize(mean=[0.5,0.5,0.5],std=[0.5,0.5,0.5],always_apply=True),\n            ToTensorV2()\n        ])\n\n\n    def save_model(self,):\n        self.train_config.model_path.mkdir(exist_ok=True)\n        sd = self.model.state_dict()\n        torch.save(self.model,self.train_config.model_path/'captioner.pt')\n\n\n    def load_best_model(self,):\n        sd = torch.load(self.train_config.model_path/'captioner.pt')\n        self.model.load_state_dict(sd)\n\n\n    def train_one_epoch(self,epoch):\n\n        prog = tqdm(self.train_dl,total=len(self.train_dl))\n\n        running_loss = 0.\n\n        for image, input_ids, labels in prog:\n\n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n\n                loss = self.model(image,input_ids,labels)\n\n                self.scaler.scale(loss).backward()\n                self.scaler.step(self.optim)\n                self.scaler.update()\n                self.sched.step()\n                self.optim.zero_grad(set_to_none=True)\n\n                running_loss += loss.item()\n\n                prog.set_description(f'train loss: {loss.item():.3f}')\n\n            del image, input_ids, labels, loss\n\n        train_loss = running_loss / len(self.train_dl)\n        train_pxp = np.exp(train_loss)\n\n        self.metrics.loc[epoch,['train_loss','train_perplexity']] = (train_loss,train_pxp)\n\n\n    @torch.no_grad()\n    def valid_one_epoch(self,epoch):\n\n        prog = tqdm(self.val_dl,total=len(self.val_dl))\n\n        running_loss = 0.\n\n        for image, input_ids, labels in prog:\n\n            with autocast():\n                image = image.to(self.device)\n                input_ids = input_ids.to(self.device)\n                labels = labels.to(self.device)\n\n                loss = self.model(image,input_ids,labels)\n                running_loss += loss.item()\n\n                prog.set_description(f'valid loss: {loss.item():.3f}')\n\n            del image, input_ids, labels, loss\n\n        val_loss = running_loss / len(self.val_dl)\n        val_pxp = np.exp(val_loss)\n\n        self.metrics.loc[epoch,['val_loss','val_perplexity']] = (val_loss,val_pxp)\n\n        return val_pxp\n\n\n    def clean(self):\n        gc.collect()\n        torch.cuda.empty_cache()\n\n\n    def fit(self,):\n\n        best_pxp = 1e9\n        best_epoch = -1\n        prog = tqdm(range(self.train_config.epochs))\n\n        for epoch in prog:\n\n            if epoch == self.train_config.freeze_epochs_gpt:\n                self.model.unfreeze_gpt_layers()\n                print('unfreezing GPT2 entirely...')\n\n            if epoch == self.train_config.freeze_epochs_all:\n                self.model.pretrained_layers_trainable(trainable=True)\n\n            self.model.train()\n            prog.set_description('training')\n            self.train_one_epoch(epoch)\n            self.clean()\n\n            self.model.eval()\n            prog.set_description('validating')\n            pxp = self.valid_one_epoch(epoch)\n            self.clean()\n\n            print(self.metrics.tail(1))\n\n            if pxp < best_pxp:\n                best_pxp = pxp\n                best_epoch = epoch\n                print('saving best model...')\n                self.save_model()\n\n        return {\n            'best_perplexity': best_pxp,\n            'best_epoch': best_epoch\n        }\n\n\n    @torch.no_grad()\n    def generate_caption(self,image,max_tokens=50,temperature=1.0,deterministic=False):\n\n        self.model.eval()\n\n        image = Image.open(image).convert('RGB')\n        image = np.array(image)\n        image = self.gen_tfms(image=image)['image']\n        image = image.unsqueeze(0).to(self.device)\n        sequence = torch.ones(1,1).to(device=self.device).long() * self.tokenizer.bos_token_id\n\n        caption = self.model.generate(\n            image,\n            sequence,\n            max_tokens=max_tokens,\n            temperature=temperature,\n            deterministic=deterministic\n        )\n        caption = self.tokenizer.decode(caption.numpy(),skip_special_tokens=True)\n\n        return caption","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:35:08.212484Z","iopub.execute_input":"2025-03-11T14:35:08.212776Z","iopub.status.idle":"2025-03-11T14:35:08.229390Z","shell.execute_reply.started":"2025-03-11T14:35:08.212755Z","shell.execute_reply":"2025-03-11T14:35:08.228499Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"train_dl = torch.utils.data.DataLoader(train_ds,batch_size=train_config.batch_size,shuffle=True,pin_memory=True,num_workers=2,persistent_workers=True,collate_fn=collate_fn)\nval_dl = torch.utils.data.DataLoader(val_ds,batch_size=train_config.batch_size,shuffle=False,pin_memory=True,num_workers=2,persistent_workers=True,collate_fn=collate_fn)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:35:12.605111Z","iopub.execute_input":"2025-03-11T14:35:12.605417Z","iopub.status.idle":"2025-03-11T14:35:12.610049Z","shell.execute_reply.started":"2025-03-11T14:35:12.605394Z","shell.execute_reply":"2025-03-11T14:35:12.608908Z"}},"outputs":[],"execution_count":34},{"cell_type":"markdown","source":"# Train ViT - without qtl","metadata":{}},{"cell_type":"code","source":"# trainer = TrainerViT(model_config,train_config,(train_dl,val_dl))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T10:39:35.646521Z","iopub.execute_input":"2025-03-04T10:39:35.646812Z","iopub.status.idle":"2025-03-04T10:39:45.407335Z","shell.execute_reply.started":"2025-03-04T10:39:35.646790Z","shell.execute_reply":"2025-03-04T10:39:45.406505Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trainer.fit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-04T10:40:34.525781Z","iopub.execute_input":"2025-03-04T10:40:34.526176Z","iopub.status.idle":"2025-03-04T11:55:01.510290Z","shell.execute_reply.started":"2025-03-04T10:40:34.526134Z","shell.execute_reply":"2025-03-04T11:55:01.508869Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# train DEiT - without qtl","metadata":{}},{"cell_type":"code","source":"trainer = TrainerDEiT(model_config,train_config,(train_dl,val_dl))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:21:26.164883Z","iopub.execute_input":"2025-03-05T16:21:26.165170Z","iopub.status.idle":"2025-03-05T16:21:35.201462Z","shell.execute_reply.started":"2025-03-05T16:21:26.165149Z","shell.execute_reply":"2025-03-05T16:21:35.200674Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e87f975bd4f1410d942e394daac09fe5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"043e57fce8e34117981e771ca3e580b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8bfde4490b6544f89dadb1482a386015"}},"metadata":{}},{"name":"stdout","text":"total_frozen_params=210236928\ntrainable parameters: 28366848\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:16: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  self.scaler = GradScaler()\n","output_type":"stream"}],"execution_count":30},{"cell_type":"code","source":"trainer.fit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T16:21:38.028833Z","iopub.execute_input":"2025-03-05T16:21:38.029162Z","iopub.status.idle":"2025-03-05T17:35:49.517862Z","shell.execute_reply.started":"2025-03-05T16:21:38.029133Z","shell.execute_reply":"2025-03-05T17:35:49.516631Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a1de2bd416e34a72a579070b7c29fa78"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"99d8a50d77b040caa5ee0bca85698481"}},"metadata":{}},{"name":"stderr","text":"You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5361b958c66d48f3a74d87fd312ee10f"}},"metadata":{}},{"name":"stderr","text":"You're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\nYou're using a GPT2TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"  train_loss train_perplexity  val_loss val_perplexity\n0  10.661588     42684.346596  8.565459    5247.248719\nsaving best model...\nunfreezing GPT2 entirely...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f2cc15dad83f4d279b5652f0f847e174"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c7a617aa3f144e7588cdc26bc07f3882"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"  train_loss train_perplexity  val_loss val_perplexity\n1   6.167197       476.847389  4.134599      62.464507\nsaving best model...\ntotal_frozen_params=0\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0af2128af2a84d7582f287a1e864d8ed"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c8f56f0dee94f09a93d5414178e3056"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"  train_loss train_perplexity  val_loss val_perplexity\n2   3.145712        23.236219  2.196557       8.993991\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aac95bcee08f4661b664f7fb1187c966"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b809b400cfad4a5b9c3932a1524db6af"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"  train_loss train_perplexity  val_loss val_perplexity\n3   2.067288         7.903363  1.693499       5.438476\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"92e1cd0d764f403fafd6fd9477eef988"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b215296666ff4e0eb476300de016d77a"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"  train_loss train_perplexity  val_loss val_perplexity\n4   1.654594         5.230958  1.427449       4.168055\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c16b23876c154cb79de6452c62a8690b"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d7c2e20893234963ad597282f9469028"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"  train_loss train_perplexity  val_loss val_perplexity\n5   1.414099         4.112777  1.274236       3.575968\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"84559fc0e7bb47058fae8956ac3a8e44"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032e21781576429e8ad084bf6da8653e"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"  train_loss train_perplexity  val_loss val_perplexity\n6   1.241835         3.461961  1.135522       3.112798\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"639c01f4e7e846b38989e0be9ce353bd"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f03f4d7be9214f7c985c5a7396a0c451"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"  train_loss train_perplexity  val_loss val_perplexity\n7   1.104156         3.016679  1.039869       2.828845\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a6674ba9e6d4269909d36433409eaed"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7d48ac02dfe47a9bf3dfb5a31f19adb"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"  train_loss train_perplexity  val_loss val_perplexity\n8   0.994236         2.702659  0.958893       2.608806\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4cc544ad95ef42e9b0d273bd6936e877"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"479126d39490439f9d0bd26fc5075ebc"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"  train_loss train_perplexity  val_loss val_perplexity\n9   0.891348         2.438415  0.891465       2.438701\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e1433ddb8a604c0b87cb91b278b31218"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"492eca9f80634ceea24138a9bc629d38"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"   train_loss train_perplexity  val_loss val_perplexity\n10   0.794105         2.212461  0.824214       2.280088\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f8c72d34575d4fbe9dc7cdbbe4f290c8"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"855002fafcd049b2a8ecd198973ab6a4"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"   train_loss train_perplexity  val_loss val_perplexity\n11   0.713203         2.040516  0.769457       2.158595\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"00fdac7089094ba68bc897f4fb9e057b"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"18d7f94afa5a4a7d8902a3b62cac7173"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"   train_loss train_perplexity  val_loss val_perplexity\n12   0.630028         1.877664  0.724339       2.063367\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b992c55169904e60ac0487450ae1787b"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5e5ace4157f46d788f6067e93ee423c"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"   train_loss train_perplexity  val_loss val_perplexity\n13   0.560106         1.750857  0.671752       1.957663\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"19a60596374e4f1697df6b867f8ab8f1"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc3d27073ff14822ab14085410d72e63"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"   train_loss train_perplexity val_loss val_perplexity\n14   0.499607         1.648074  0.63934        1.89523\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b07ffbbb29d4e8b8e970aa6eafc8c9e"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f27fb822ac1548a58e8315edbb7d70ab"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"   train_loss train_perplexity  val_loss val_perplexity\n15   0.448677         1.566238  0.613777       1.847396\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"29331212655e405cbf920c75e8b0d3f5"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f381b9e95014ccc93740192bba0f4c2"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"   train_loss train_perplexity val_loss val_perplexity\n16   0.412221         1.510167   0.5972       1.817024\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8797afe671a848d09898ad8c0a56afe8"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca8d9f232254454087bb2f415a5ac0ab"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"   train_loss train_perplexity  val_loss val_perplexity\n17    0.38872         1.475092  0.587717       1.799875\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c092c32c2d547ecbff820d4fdcd3f4c"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6aec9acee19d443885a0045a322a6a21"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"   train_loss train_perplexity  val_loss val_perplexity\n18   0.375021         1.455022  0.581438       1.788609\nsaving best model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/198 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"285b2e6631794c12b8197b0b55f2d39c"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:61: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/22 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f59b143b277e41e984e9fc789bb09900"}},"metadata":{}},{"name":"stderr","text":"<ipython-input-27-631439bfcab2>:95: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():\n","output_type":"stream"},{"name":"stdout","text":"   train_loss train_perplexity  val_loss val_perplexity\n19   0.367138         1.443597  0.581269       1.788307\nsaving best model...\n","output_type":"stream"},{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"{'best_perplexity': 1.7883066454516778, 'best_epoch': 19}"},"metadata":{}}],"execution_count":31},{"cell_type":"markdown","source":"# train BEiT - without qtl","metadata":{}},{"cell_type":"code","source":"# trainer = TrainerBEiT(model_config,train_config,(train_dl,val_dl))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T04:41:54.542695Z","iopub.execute_input":"2025-03-05T04:41:54.543022Z","iopub.status.idle":"2025-03-05T04:42:04.818299Z","shell.execute_reply.started":"2025-03-05T04:41:54.543000Z","shell.execute_reply":"2025-03-05T04:42:04.817369Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# trainer.fit()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T04:42:31.562468Z","iopub.execute_input":"2025-03-05T04:42:31.562811Z","iopub.status.idle":"2025-03-05T05:56:35.175193Z","shell.execute_reply.started":"2025-03-05T04:42:31.562782Z","shell.execute_reply":"2025-03-05T05:56:35.174017Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\n# Load the model and move it to the appropriate device\nDEiTModel = torch.load('/kaggle/input/captioners/pytorch/default/1/DEiTQTL_captioner.pt')\nViTModel = torch.load('/kaggle/input/captioners/pytorch/default/1/ViTQTL_captioner.pt')\nBEiTModel = torch.load('/kaggle/input/captioners/pytorch/default/1/BEiTQTL_captioner.pt')\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nDEiTModel.to(device)\nViTModel.to(device)\nBEiTModel.to(device)\n","metadata":{"id":"0c7zXn4GB40k","outputId":"0840e32f-6579-4312-ecc1-c73192dbdd96","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:35:58.765034Z","iopub.execute_input":"2025-03-11T14:35:58.765379Z","iopub.status.idle":"2025-03-11T14:36:19.681420Z","shell.execute_reply.started":"2025-03-11T14:35:58.765350Z","shell.execute_reply":"2025-03-11T14:36:19.680641Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-35-f0274bbe9c9b>:3: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  DEiTModel = torch.load('/kaggle/input/captioners/pytorch/default/1/DEiTQTL_captioner.pt')\n<ipython-input-35-f0274bbe9c9b>:4: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  ViTModel = torch.load('/kaggle/input/captioners/pytorch/default/1/ViTQTL_captioner.pt')\n<ipython-input-35-f0274bbe9c9b>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  BEiTModel = torch.load('/kaggle/input/captioners/pytorch/default/1/BEiTQTL_captioner.pt')\n","output_type":"stream"},{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"BEiTGPT2Model(\n  (patch_embed): PatchEmbed(\n    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n    (norm): Identity()\n  )\n  (pos_drop): Dropout(p=0.0, inplace=False)\n  (blocks): ModuleList(\n    (0-11): 12 x Block(\n      (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (attn): Attention(\n        (qkv): Linear(in_features=768, out_features=2304, bias=True)\n        (q_norm): Identity()\n        (k_norm): Identity()\n        (attn_drop): Dropout(p=0.0, inplace=False)\n        (proj): Linear(in_features=768, out_features=768, bias=True)\n        (proj_drop): Dropout(p=0.0, inplace=False)\n      )\n      (ls1): Identity()\n      (drop_path1): Identity()\n      (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n      (mlp): Mlp(\n        (fc1): Linear(in_features=768, out_features=3072, bias=True)\n        (act): GELU(approximate='none')\n        (drop1): Dropout(p=0.0, inplace=False)\n        (norm): Identity()\n        (fc2): Linear(in_features=3072, out_features=768, bias=True)\n        (drop2): Dropout(p=0.0, inplace=False)\n      )\n      (ls2): Identity()\n      (drop_path2): Identity()\n    )\n  )\n  (transformer): ModuleDict(\n    (wte): Embedding(50257, 768)\n    (wpe): Embedding(1024, 768)\n    (drop): Dropout(p=0.1, inplace=False)\n    (h): ModuleList(\n      (0-11): 12 x GPT2Block(\n        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (attn): GPT2Attention(\n          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (mlp): GPT2MLP(\n          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n          (act): GELU(approximate='none')\n          (dropout): Dropout(p=0.1, inplace=False)\n        )\n        (ln_3): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n        (cross_attn): GPT2CrossAttention(\n          (q): Linear(in_features=768, out_features=768, bias=True)\n          (k): Linear(in_features=768, out_features=768, bias=True)\n          (v): Linear(in_features=768, out_features=768, bias=True)\n          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n          (attn_dropout): Dropout(p=0.1, inplace=False)\n          (resid_dropout): Dropout(p=0.1, inplace=False)\n        )\n      )\n    )\n    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n  )\n  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n)"},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"def generate_caption( image, max_tokens=200, temperature=1.0, deterministic=False):\n    # model.eval()\n    gen_tfms = A.Compose([\n        A.Resize(224, 224),\n        A.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5], always_apply=True),\n        ToTensorV2()\n    ])\n    image = Image.open(image).convert('RGB')\n    image = np.array(image)\n    image = gen_tfms(image=image)['image']\n    image = image.unsqueeze(0).to(device)  # Move the input image tensor to the same device as the model\n    sequence = torch.ones(1, 1).long().to(device) * tokenizer.bos_token_id\n\n    caption1 = DEiTModel.generate(image,sequence,max_tokens=max_tokens,temperature=temperature,deterministic=deterministic)\n    caption2 = ViTModel.generate(image,sequence,max_tokens=max_tokens,temperature=temperature,deterministic=deterministic)\n    caption3 = BEiTModel.generate(image,sequence,max_tokens=max_tokens,temperature=temperature,deterministic=deterministic)\n\n    caption1 = tokenizer.decode(caption1.cpu().numpy(), skip_special_tokens=True)  # Move the generated caption back to CPU for decoding\n    caption2 = tokenizer.decode(caption2.cpu().numpy(), skip_special_tokens=True)\n    caption3 = tokenizer.decode(caption3.cpu().numpy(), skip_special_tokens=True)\n    return caption1,caption2,caption3","metadata":{"id":"N6od9Er9B-Tn","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:36:21.415397Z","iopub.execute_input":"2025-03-11T14:36:21.415702Z","iopub.status.idle":"2025-03-11T14:36:21.422261Z","shell.execute_reply.started":"2025-03-11T14:36:21.415677Z","shell.execute_reply":"2025-03-11T14:36:21.421444Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import GPT2Tokenizer, GPT2Model\nfrom sklearn.metrics.pairwise import cosine_similarity\ndef get_similarity(gen_cap,pred_cap):\n  tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n  tokenizer.pad_token = tokenizer.eos_token\n  Embedding_model = GPT2Model.from_pretrained('gpt2')\n  pred_tokens = [tokenizer.encode(pred_cap[0], return_tensors=\"pt\", padding=False, truncation=True)]\n  gen_tokens = [tokenizer.encode(sentence, return_tensors=\"pt\", padding=False, truncation=True) for sentence in gen_cap]\n  # Obtain embeddings for predicted and actual sentences\n  with torch.no_grad():\n      pred_outputs = [Embedding_model(pred_token) for pred_token in pred_tokens]\n      gen_outputs = [Embedding_model(gen_token) for gen_token in gen_tokens]\n\n  # Extract embeddings from model outputs\n  pred_embeddings = [output.last_hidden_state[:, 0, :].numpy() for output in pred_outputs]\n  gen_embeddings = [output.last_hidden_state[:, 0, :].numpy() for output in gen_outputs]\n  cs1 = cosine_similarity(pred_embeddings[0], gen_embeddings[0])[0][0]\n  cs2 = cosine_similarity(pred_embeddings[0], gen_embeddings[1])[0][0]\n  cs3 = cosine_similarity(pred_embeddings[0], gen_embeddings[2])[0][0]\n  return [cs1,cs2,cs3]","metadata":{"id":"ZUxyUS4rCrUt","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:36:28.483329Z","iopub.execute_input":"2025-03-11T14:36:28.483699Z","iopub.status.idle":"2025-03-11T14:36:28.490505Z","shell.execute_reply.started":"2025-03-11T14:36:28.483671Z","shell.execute_reply":"2025-03-11T14:36:28.489604Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"pip install ragas","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-06T17:48:52.601919Z","iopub.execute_input":"2025-03-06T17:48:52.602240Z","iopub.status.idle":"2025-03-06T17:49:05.338968Z","shell.execute_reply.started":"2025-03-06T17:48:52.602216Z","shell.execute_reply":"2025-03-06T17:49:05.337952Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting ragas\n  Downloading ragas-0.2.14-py3-none-any.whl.metadata (8.5 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ragas) (1.26.4)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from ragas) (3.3.1)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from ragas) (0.9.0)\nRequirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from ragas) (0.3.12)\nRequirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (from ragas) (0.3.25)\nCollecting langchain-community (from ragas)\n  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\nCollecting langchain_openai (from ragas)\n  Downloading langchain_openai-0.3.7-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas) (1.6.0)\nCollecting appdirs (from ragas)\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: pydantic>=2 in /usr/local/lib/python3.10/dist-packages (from ragas) (2.11.0a2)\nRequirement already satisfied: openai>1 in /usr/local/lib/python3.10/dist-packages (from ragas) (1.57.4)\nCollecting diskcache>=5.6.3 (from ragas)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (0.8.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (4.12.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ragas) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ragas) (2.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->ragas) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (6.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->ragas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->ragas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->ragas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->ragas) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->ragas) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->ragas) (2.4.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (2.0.36)\nCollecting async-timeout<5.0.0,>=4.0.0 (from langchain->ragas)\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (0.3.3)\nRequirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (0.2.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (9.0.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core->ragas) (1.33)\nCollecting langchain-core (from ragas)\n  Downloading langchain_core-0.3.41-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain (from ragas)\n  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community->ragas)\n  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community->ragas)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain->ragas)\n  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\nCollecting openai>1 (from ragas)\n  Downloading openai-1.65.4-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->ragas) (2024.11.6)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.18.3)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.10)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas) (1.2.2)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain->ragas) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain->ragas) (1.0.0)\nCollecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community->ragas)\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->ragas) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->ragas) (2.3.0)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.1.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->ragas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->ragas) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->ragas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->ragas) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2025.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->ragas) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.17.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\nDownloading ragas-0.2.14-py3-none-any.whl (187 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nDownloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m37.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.41-py3-none-any.whl (415 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.1/415.1 kB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_openai-0.3.7-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.3/55.3 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading openai-1.65.4-py3-none-any.whl (473 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.5/473.5 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\nDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\nDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nInstalling collected packages: appdirs, python-dotenv, httpx-sse, diskcache, async-timeout, pydantic-settings, openai, langchain-core, langchain-text-splitters, langchain_openai, langchain, langchain-community, ragas\n  Attempting uninstall: async-timeout\n    Found existing installation: async-timeout 5.0.1\n    Uninstalling async-timeout-5.0.1:\n      Successfully uninstalled async-timeout-5.0.1\n  Attempting uninstall: openai\n    Found existing installation: openai 1.57.4\n    Uninstalling openai-1.57.4:\n      Successfully uninstalled openai-1.57.4\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.25\n    Uninstalling langchain-core-0.3.25:\n      Successfully uninstalled langchain-core-0.3.25\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.3\n    Uninstalling langchain-text-splitters-0.3.3:\n      Successfully uninstalled langchain-text-splitters-0.3.3\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.12\n    Uninstalling langchain-0.3.12:\n      Successfully uninstalled langchain-0.3.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed appdirs-1.4.4 async-timeout-4.0.3 diskcache-5.6.3 httpx-sse-0.4.0 langchain-0.3.20 langchain-community-0.3.19 langchain-core-0.3.41 langchain-text-splitters-0.3.6 langchain_openai-0.3.7 openai-1.65.4 pydantic-settings-2.8.1 python-dotenv-1.0.1 ragas-0.2.14\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":35},{"cell_type":"code","source":"# from ragas import evaluate\n# from datasets import Dataset\n# from ragas.metrics import (\n#     faithfulness,\n#     answer_similarity,\n#     answer_correctness\n# )\n\n# def get_rag(gen_cap,pred_cap):\n#   r1 = [pred_cap[0] for i in range(3)]\n#   ds = Dataset.from_pandas(pd.DataFrame({'ground_truth':r1,'answer':gen_cap}))\n\n#   result = evaluate( dataset = ds, metrics=[ answer_similarity,answer_correctness]).to_pandas()\n#   return result[\"answer_similarity\"],result[\"answer_correctness\"]","metadata":{"id":"_eWNK9FSqMDM","trusted":true,"execution":{"iopub.status.busy":"2025-03-06T17:49:16.611702Z","iopub.execute_input":"2025-03-06T17:49:16.612085Z","iopub.status.idle":"2025-03-06T17:49:18.900015Z","shell.execute_reply.started":"2025-03-06T17:49:16.612051Z","shell.execute_reply":"2025-03-06T17:49:18.899233Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"### **Test Case 1**","metadata":{"id":"dv8xfm_jBDDY"}},{"cell_type":"code","source":"# import textwrap\n# import matplotlib.image as mpimg\n# import matplotlib.pyplot as plt\n# from IPython.core.display import display, HTML\n# path = \"/kaggle/working/xray_images/CXR3138_IM-1476-1001.png\"\n# img = mpimg.imread(path)\n# plt.imshow(img)\n# plt.axis('off')  # Hide axes\n# plt.show()\n# orig = dataFrame[dataFrame[\"image\"]==path][\"caption\"].values\n# caption1,caption2,caption3 = generate_caption(path)\n# captions= [caption1,caption2,caption3]\n# stcs_values = get_similarity(captions,orig)\n# similarity_values = get_rag(captions, orig)[0]\n# models = [\"ViTGPT2\",\"DEiTGPT2\",\"BEiTGPT2\"]\n# print(\"\\nOriginal Finding: \",textwrap.fill(orig[0],width=90),\"\\n\")\n# for i in range(3):\n#   print(models[i])\n#   finding = textwrap.fill(captions[i], width=90)\n#   print(\"Findings: \",finding)\n#   print(\"Skip Thought CS Value: \",stcs_values[i])\n#   print(\"RAG_answer_similarity: \",similarity_values[i])\n#   # print(\"RAG_answer_correctness: \",answer_correctness[i])\n#   print(\"\\n\")\n","metadata":{"id":"D3LMCzi3Mb6L","outputId":"4e00ae46-61de-43bf-bbc3-48a92932166b","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:21:07.915618Z","iopub.execute_input":"2025-03-11T14:21:07.915907Z","iopub.status.idle":"2025-03-11T14:21:07.919923Z","shell.execute_reply.started":"2025-03-11T14:21:07.915883Z","shell.execute_reply":"2025-03-11T14:21:07.918918Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"# print(get_rag(captions, orig))\n","metadata":{"id":"3_sOzUvTDkN4","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:21:08.695754Z","iopub.execute_input":"2025-03-11T14:21:08.696055Z","iopub.status.idle":"2025-03-11T14:21:08.699599Z","shell.execute_reply.started":"2025-03-11T14:21:08.696031Z","shell.execute_reply":"2025-03-11T14:21:08.698829Z"}},"outputs":[],"execution_count":35},{"cell_type":"code","source":"# path = \"/kaggle/working/xray_images/CXR1022_IM-0017-2001.png\"\n# img = mpimg.imread(path)\n# plt.imshow(img)\n# plt.axis('off')  # Hide axes\n# plt.show()\n# orig = dataFrame[dataFrame[\"image\"]==path][\"caption\"].values\n# caption1,caption2,caption3 = generate_caption(path)\n# captions= [caption1,caption2,caption3]\n# stcs_values = get_similarity(captions,orig)\n# similarity_values,answer_correctness = get_rag(captions,orig)\n# models = [\"ViTGPT2\",\"DEiTGPT2\",\"BEiTGPT2\"]\n# print(\"\\nOriginal Finding: \",textwrap.fill(orig[0],width=90),\"\\n\")\n# for i in range(3):\n#   print(models[i])\n#   finding = textwrap.fill(captions[i], width=90)\n#   print(\"Findings: \",finding)\n#   print(\"Skip Thought CS Value: \",stcs_values[i])\n#   print(\"RAG_answer_similarity: \",similarity_values[i])\n#   print(\"RAG_answer_correctness: \",answer_correctness[i])\n#   print(\"\\n\")\n","metadata":{"id":"vv4fnibsBc8K","outputId":"a97b9e02-2e09-4107-e3de-416d2a379d3a","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:21:09.509749Z","iopub.execute_input":"2025-03-11T14:21:09.510052Z","iopub.status.idle":"2025-03-11T14:21:09.513391Z","shell.execute_reply.started":"2025-03-11T14:21:09.510029Z","shell.execute_reply":"2025-03-11T14:21:09.512570Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# path = \"/content/drive/MyDrive/X_ray_Report_generation/xray_images/CXR152_IM-0335-1001.png\"\n# img = mpimg.imread(path)\n# plt.imshow(img)\n# plt.axis('off')  # Hide axes\n# plt.show()\n# orig = dataFrame[dataFrame[\"image\"]==path][\"caption\"].values\n# caption1,caption2,caption3 = generate_caption(path)\n# captions= [caption1,caption2,caption3]\n# stcs_values = get_similarity(captions,orig)\n# similarity_values,answer_correctness = get_rag(captions,orig)\n# models = [\"ViTGPT2\",\"DEiTGPT2\",\"BEiTGPT2\"]\n# print(\"\\nOriginal Finding: \",textwrap.fill(orig[0],width=90),\"\\n\")\n# for i in range(3):\n#   print(models[i])\n#   finding = textwrap.fill(captions[i], width=90)\n#   print(\"Findings: \",finding)\n#   print(\"Skip Thought CS Value: \",stcs_values[i])\n#   print(\"RAG_answer_similarity: \",similarity_values[i])\n#   print(\"RAG_answer_correctness: \",answer_correctness[i])\n#   print(\"\\n\")","metadata":{"id":"0rOMWMDiff96","outputId":"d87408af-0e73-442c-d07e-d0de4cc1d47e","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:21:10.305704Z","iopub.execute_input":"2025-03-11T14:21:10.306014Z","iopub.status.idle":"2025-03-11T14:21:10.309834Z","shell.execute_reply.started":"2025-03-11T14:21:10.305990Z","shell.execute_reply":"2025-03-11T14:21:10.308858Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"\n# path = \"/content/drive/MyDrive/X_ray_Report_generation/xray_images/CXR252_IM-1038-1001.png\"\n# img = mpimg.imread(path)\n# plt.imshow(img)\n# plt.axis('off')  # Hide axes\n# plt.show()\n# orig = dataFrame[dataFrame[\"image\"]==path][\"caption\"].values\n# caption1,caption2,caption3 = generate_caption(path)\n# captions= [caption1,caption2,caption3]\n# stcs_values = get_similarity(captions,orig)\n# similarity_values,answer_correctness = get_rag(captions,orig)\n# models = [\"ViTGPT2\",\"DEiTGPT2\",\"BEiTGPT2\"]\n# print(\"\\nOriginal Finding: \",textwrap.fill(orig[0],width=90),\"\\n\")\n# for i in range(3):\n#   print(models[i])\n#   finding = textwrap.fill(captions[i], width=90)\n#   print(\"Findings: \",finding)\n#   print(\"Skip Thought CS Value: \",stcs_values[i])\n#   print(\"RAG_answer_similarity: \",similarity_values[i])\n#   print(\"RAG_answer_correctness: \",answer_correctness[i])\n#   print(\"\\n\")\n","metadata":{"id":"WFwd1TIBfgAP","outputId":"f1893c97-6e0b-42b8-c9ef-00b525285216","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:21:11.106674Z","iopub.execute_input":"2025-03-11T14:21:11.106960Z","iopub.status.idle":"2025-03-11T14:21:11.110695Z","shell.execute_reply.started":"2025-03-11T14:21:11.106937Z","shell.execute_reply":"2025-03-11T14:21:11.109781Z"}},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":"## **Validation**","metadata":{"id":"4VRZkUc4Mcr6"}},{"cell_type":"code","source":"test_cap = []\nDEiT_gen_cap = []\nViT_gen_cap = []\nBEiT_gen_cap = []\nfrom tqdm import tqdm\nfor i in tqdm(range(2), desc=\"Processing elements\", unit=\"element\"):\n    det = False\n    test = val_df.sample(n=1).values[0]\n    test_img, test_caption = test[0],test[1]\n    t = np.random.uniform(0.5,1.5)\n    if i > 40:\n        det = True\n    try:\n      c1,c2,c3 = generate_caption(test_img,temperature=t,deterministic=det)\n      test_cap.append(test_caption)\n      DEiT_gen_cap.append(c1)\n      ViT_gen_cap.append(c2)\n      BEiT_gen_cap.append(c3)\n    except:\n      pass","metadata":{"id":"fFxhZCdJCA4k","outputId":"e1aef174-8abb-410c-c92b-642de76a4f7b","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:36:41.331603Z","iopub.execute_input":"2025-03-11T14:36:41.332034Z","iopub.status.idle":"2025-03-11T14:36:49.010293Z","shell.execute_reply.started":"2025-03-11T14:36:41.332000Z","shell.execute_reply":"2025-03-11T14:36:49.009351Z"}},"outputs":[{"name":"stderr","text":"Processing elements: 100%|██████████| 2/2 [00:07<00:00,  3.83s/element]\n","output_type":"stream"}],"execution_count":38},{"cell_type":"code","source":"vit_gen_cap = [\"findings - \"+i for i in ViT_gen_cap]","metadata":{"id":"KD5zOdXVS0c5","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:36:52.590752Z","iopub.execute_input":"2025-03-11T14:36:52.591107Z","iopub.status.idle":"2025-03-11T14:36:52.595123Z","shell.execute_reply.started":"2025-03-11T14:36:52.591080Z","shell.execute_reply":"2025-03-11T14:36:52.594160Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"df_vit = pd.DataFrame({'ground_truth':test_cap,'answer':vit_gen_cap})","metadata":{"id":"9zXQVtVpMxjS","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:36:54.067305Z","iopub.execute_input":"2025-03-11T14:36:54.067774Z","iopub.status.idle":"2025-03-11T14:36:54.073300Z","shell.execute_reply.started":"2025-03-11T14:36:54.067733Z","shell.execute_reply":"2025-03-11T14:36:54.072206Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"!pip install datasets","metadata":{"id":"WCzCzOz-TswB","outputId":"2bb78653-dac7-4062-c7c2-6939d1519a10","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:36:55.855504Z","iopub.execute_input":"2025-03-11T14:36:55.855790Z","iopub.status.idle":"2025-03-11T14:36:59.280016Z","shell.execute_reply.started":"2025-03-11T14:36:55.855768Z","shell.execute_reply":"2025-03-11T14:36:59.278900Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.3.1)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.17.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2025.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy>=1.17->datasets) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"from datasets import Dataset\ndataset = Dataset.from_pandas(df_vit[:2])","metadata":{"id":"kLtu_xldTox4","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:36:59.281600Z","iopub.execute_input":"2025-03-11T14:36:59.281962Z","iopub.status.idle":"2025-03-11T14:37:00.098156Z","shell.execute_reply.started":"2025-03-11T14:36:59.281930Z","shell.execute_reply":"2025-03-11T14:37:00.097499Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"dataset","metadata":{"id":"Cl0wz4epTo7I","outputId":"11bd1a3a-631f-486b-b0e5-069ab98fa27b","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:37:01.092577Z","iopub.execute_input":"2025-03-11T14:37:01.093337Z","iopub.status.idle":"2025-03-11T14:37:01.098937Z","shell.execute_reply.started":"2025-03-11T14:37:01.093301Z","shell.execute_reply":"2025-03-11T14:37:01.098042Z"}},"outputs":[{"execution_count":43,"output_type":"execute_result","data":{"text/plain":"Dataset({\n    features: ['ground_truth', 'answer'],\n    num_rows: 2\n})"},"metadata":{}}],"execution_count":43},{"cell_type":"code","source":"!pip install ragas  python-dotenv","metadata":{"id":"Ro_rLxlgSRFI","outputId":"e0e00119-cede-4358-d267-e63ee4de31f6","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:21:46.585321Z","iopub.execute_input":"2025-03-11T14:21:46.585666Z","iopub.status.idle":"2025-03-11T14:21:57.532909Z","shell.execute_reply.started":"2025-03-11T14:21:46.585622Z","shell.execute_reply":"2025-03-11T14:21:57.531863Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stdout","text":"Collecting ragas\n  Downloading ragas-0.2.14-py3-none-any.whl.metadata (8.5 kB)\nCollecting python-dotenv\n  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from ragas) (1.26.4)\nRequirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from ragas) (3.3.1)\nRequirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from ragas) (0.9.0)\nRequirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (from ragas) (0.3.12)\nRequirement already satisfied: langchain-core in /usr/local/lib/python3.10/dist-packages (from ragas) (0.3.25)\nCollecting langchain-community (from ragas)\n  Downloading langchain_community-0.3.19-py3-none-any.whl.metadata (2.4 kB)\nCollecting langchain_openai (from ragas)\n  Downloading langchain_openai-0.3.8-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from ragas) (1.6.0)\nCollecting appdirs (from ragas)\n  Downloading appdirs-1.4.4-py2.py3-none-any.whl.metadata (9.0 kB)\nRequirement already satisfied: pydantic>=2 in /usr/local/lib/python3.10/dist-packages (from ragas) (2.11.0a2)\nRequirement already satisfied: openai>1 in /usr/local/lib/python3.10/dist-packages (from ragas) (1.57.4)\nCollecting diskcache>=5.6.3 (from ragas)\n  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (0.8.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (4.67.1)\nRequirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.10/dist-packages (from openai>1->ragas) (4.12.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ragas) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2->ragas) (2.29.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (2.32.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.70.16)\nRequirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets->ragas) (2024.12.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (3.11.12)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (0.29.0)\nRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (24.2)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets->ragas) (6.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy->ragas) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy->ragas) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy->ragas) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy->ragas) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy->ragas) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy->ragas) (2.4.1)\nRequirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (2.0.36)\nCollecting async-timeout<5.0.0,>=4.0.0 (from langchain->ragas)\n  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\nRequirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (0.3.3)\nRequirement already satisfied: langsmith<0.3,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (0.2.3)\nRequirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->ragas) (9.0.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core->ragas) (1.33)\nCollecting langchain-core (from ragas)\n  Downloading langchain_core-0.3.43-py3-none-any.whl.metadata (5.9 kB)\nCollecting langchain (from ragas)\n  Downloading langchain-0.3.20-py3-none-any.whl.metadata (7.7 kB)\nRequirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain-community->ragas) (0.6.7)\nCollecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community->ragas)\n  Downloading pydantic_settings-2.8.1-py3-none-any.whl.metadata (3.5 kB)\nCollecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community->ragas)\n  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\nCollecting langchain-text-splitters<1.0.0,>=0.3.6 (from langchain->ragas)\n  Downloading langchain_text_splitters-0.3.6-py3-none-any.whl.metadata (1.9 kB)\nCollecting openai>1 (from ragas)\n  Downloading openai-1.65.5-py3-none-any.whl.metadata (27 kB)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->ragas) (2024.11.6)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (2.4.6)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->ragas) (1.18.3)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas) (3.10)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai>1->ragas) (1.2.2)\nRequirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (3.26.1)\nRequirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (0.9.0)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai>1->ragas) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>1->ragas) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core->ragas) (3.0.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain->ragas) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.3,>=0.1.17->langchain->ragas) (1.0.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->ragas) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets->ragas) (2.3.0)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->ragas) (3.1.1)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->ragas) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy->ragas) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy->ragas) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy->ragas) (2024.2.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->ragas) (2025.1)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy->ragas) (2024.2.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets->ragas) (1.17.0)\nRequirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community->ragas) (1.0.0)\nDownloading ragas-0.2.14-py3-none-any.whl (187 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\nDownloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\nDownloading langchain_community-0.3.19-py3-none-any.whl (2.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[?25hDownloading langchain-0.3.20-py3-none-any.whl (1.0 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m45.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_core-0.3.43-py3-none-any.whl (415 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.4/415.4 kB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading langchain_openai-0.3.8-py3-none-any.whl (55 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.4/55.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading openai-1.65.5-py3-none-any.whl (474 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m474.5/474.5 kB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\nDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\nDownloading langchain_text_splitters-0.3.6-py3-none-any.whl (31 kB)\nDownloading pydantic_settings-2.8.1-py3-none-any.whl (30 kB)\nInstalling collected packages: appdirs, python-dotenv, httpx-sse, diskcache, async-timeout, pydantic-settings, openai, langchain-core, langchain-text-splitters, langchain_openai, langchain, langchain-community, ragas\n  Attempting uninstall: async-timeout\n    Found existing installation: async-timeout 5.0.1\n    Uninstalling async-timeout-5.0.1:\n      Successfully uninstalled async-timeout-5.0.1\n  Attempting uninstall: openai\n    Found existing installation: openai 1.57.4\n    Uninstalling openai-1.57.4:\n      Successfully uninstalled openai-1.57.4\n  Attempting uninstall: langchain-core\n    Found existing installation: langchain-core 0.3.25\n    Uninstalling langchain-core-0.3.25:\n      Successfully uninstalled langchain-core-0.3.25\n  Attempting uninstall: langchain-text-splitters\n    Found existing installation: langchain-text-splitters 0.3.3\n    Uninstalling langchain-text-splitters-0.3.3:\n      Successfully uninstalled langchain-text-splitters-0.3.3\n  Attempting uninstall: langchain\n    Found existing installation: langchain 0.3.12\n    Uninstalling langchain-0.3.12:\n      Successfully uninstalled langchain-0.3.12\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed appdirs-1.4.4 async-timeout-4.0.3 diskcache-5.6.3 httpx-sse-0.4.0 langchain-0.3.20 langchain-community-0.3.19 langchain-core-0.3.43 langchain-text-splitters-0.3.6 langchain_openai-0.3.8 openai-1.65.5 pydantic-settings-2.8.1 python-dotenv-1.0.1 ragas-0.2.14\n","output_type":"stream"}],"execution_count":45},{"cell_type":"markdown","source":"### **DEITGPT2**","metadata":{"id":"34Hhl-XICAi-"}},{"cell_type":"code","source":"import numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers import GPT2Tokenizer, GPT2Model\nfrom sklearn.metrics.pairwise import cosine_similarity\n\n\n# Initialize GPT2 tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\nEmbedding_model = GPT2Model.from_pretrained('gpt2')\n\n# Example predicted and actual sentences\npred_cap = test_cap  # Replace with your predicted sentences\ngen_cap = DEiT_gen_cap         # Replace with your actual sentences\n\n# Tokenize predicted and actual sentences\npred_tokens = [tokenizer.encode(sentence, return_tensors=\"pt\", padding=False, truncation=True) for sentence in pred_cap]\ngen_tokens = [tokenizer.encode(sentence, return_tensors=\"pt\", padding=False, truncation=True) for sentence in gen_cap]\n\n# Obtain embeddings for predicted and actual sentences\nwith torch.no_grad():\n    pred_outputs = [Embedding_model(pred_token) for pred_token in pred_tokens]\n    gen_outputs = [Embedding_model(gen_token) for gen_token in gen_tokens]\n\n# Extract embeddings from model outputs\npred_embeddings = [output.last_hidden_state[:, 0, :].numpy() for output in pred_outputs]\ngen_embeddings = [output.last_hidden_state[:, 0, :].numpy() for output in gen_outputs]","metadata":{"id":"FwCFWRRRFGNS","outputId":"e9621cbc-8849-4c17-d854-6ebb9e75dc50","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:37:10.628712Z","iopub.execute_input":"2025-03-11T14:37:10.629074Z","iopub.status.idle":"2025-03-11T14:37:16.327653Z","shell.execute_reply.started":"2025-03-11T14:37:10.629047Z","shell.execute_reply":"2025-03-11T14:37:16.326733Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f156adb7ea204e15910d877f688b1a28"}},"metadata":{}}],"execution_count":44},{"cell_type":"code","source":"cosine_similarity_scores = []\nfor pred_embedding, gen_embedding in zip(pred_embeddings, gen_embeddings):\n    cosine_similarity_score = cosine_similarity(pred_embedding, gen_embedding)[0][0]\n    cosine_similarity_scores.append(cosine_similarity_score)\nprint(\"Skip thought CS: \",np.mean(cosine_similarity_scores))","metadata":{"id":"JVuw1dXmFdTM","outputId":"992d8a7e-7efd-4443-f06e-871540375f32","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:37:19.073004Z","iopub.execute_input":"2025-03-11T14:37:19.073306Z","iopub.status.idle":"2025-03-11T14:37:19.080346Z","shell.execute_reply.started":"2025-03-11T14:37:19.073283Z","shell.execute_reply":"2025-03-11T14:37:19.079379Z"}},"outputs":[{"name":"stdout","text":"Skip thought CS:  0.9960423\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"greedy_matching_scores = []\nfor pred_embedding, gen_embedding in zip(pred_embeddings, gen_embeddings):\n    greedy_matching_score = np.corrcoef(pred_embedding,gen_embedding)\n    greedy_matching_scores.append(greedy_matching_score)\nprint(\"Greedy matching scores between predicted and actual sentences:\", np.mean(greedy_matching_scores))","metadata":{"id":"TpO0f3UmGLeN","outputId":"a11e55de-8aec-41a7-a444-57db8ccda56f","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:37:23.957854Z","iopub.execute_input":"2025-03-11T14:37:23.958181Z","iopub.status.idle":"2025-03-11T14:37:23.972014Z","shell.execute_reply.started":"2025-03-11T14:37:23.958156Z","shell.execute_reply":"2025-03-11T14:37:23.971128Z"}},"outputs":[{"name":"stdout","text":"Greedy matching scores between predicted and actual sentences: 0.9980238213589177\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"pred_extrema_embeddings = [[np.min(embedding), np.max(embedding)] for embedding in pred_embeddings]\ngen_extrema_embeddings = [[np.min(embedding), np.max(embedding)] for embedding in gen_embeddings]\nres = []\nfor v1,v2 in zip(pred_extrema_embeddings, gen_extrema_embeddings):\n  v1 = np.array(v1)\n  v2 = np.array(v2)\n  res.append(cosine_similarity(v1.reshape(1,-1), v2.reshape(1,-1))[0][0])\n  # break\nprint(\"Vector Extrema: \",np.mean(res))","metadata":{"id":"uLg48FOCGQuo","outputId":"587f1dad-95a8-468d-9241-ef93f0d6a6f9","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:37:29.040692Z","iopub.execute_input":"2025-03-11T14:37:29.041067Z","iopub.status.idle":"2025-03-11T14:37:29.048963Z","shell.execute_reply.started":"2025-03-11T14:37:29.041039Z","shell.execute_reply":"2025-03-11T14:37:29.047950Z"}},"outputs":[{"name":"stdout","text":"Vector Extrema:  0.9987339\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"\n# from nltk.translate.bleu_score import corpus_bleu\n\n# def calculate_bleu_evaluation(GT_sentences, predicted_sentences):\n#     BLEU_1 = corpus_bleu(GT_sentences, predicted_sentences, weights=w1)\n#     BLEU_2 = corpus_bleu(GT_sentences, predicted_sentences, weights=w2)\n#     BLEU_3 = corpus_bleu(GT_sentences, predicted_sentences, weights=w3)\n#     BLEU_4 = corpus_bleu(GT_sentences, predicted_sentences, weights=w4)\n\n#     return BLEU_1,BLEU_2,BLEU_3,BLEU_4\n\n# # Define BLEU score weights\n# w1 = (1.0, 0, 0, 0)  # BLEU-1 (Unigram precision)\n# w2 = (0.5, 0.5, 0, 0)  # BLEU-2 (Bigram precision)\n# w3 = (0.33, 0.33, 0.33, 0)  # BLEU-3 (Trigram precision)\n# w4 = (0.25, 0.25, 0.25, 0.25)  # BLEU-4 (4-gram precision)\n\n# bleu1_scores, bleu2_scores, bleu3_scores, bleu4_scores = calculate_bleu_evaluation(DEiT_gen_cap, test_cap)\n# bleu1_scores = np.mean(bleu1_scores)\n# bleu2_scores = np.mean(bleu2_scores)\n# bleu3_scores = np.mean(bleu3_scores)\n# bleu4_scores = np.mean(bleu4_scores)\n\n\n# print(f\"BLEU-1 score: {bleu1_scores:.5f}\")\n# print(f\"BLEU-2 score: {bleu2_scores:.5f}\")\n# print(f\"BLEU-3 score: {bleu3_scores:.5f}\")\n# print(f\"BLEU-4 score: {bleu4_scores:.5f}\")\n","metadata":{"id":"4OS_HNieVE7C","outputId":"ecc1b80f-92a9-4ed6-eea8-377f54a40f8f","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:22:10.349862Z","iopub.execute_input":"2025-03-11T14:22:10.350149Z","iopub.status.idle":"2025-03-11T14:22:10.353842Z","shell.execute_reply.started":"2025-03-11T14:22:10.350127Z","shell.execute_reply":"2025-03-11T14:22:10.352917Z"}},"outputs":[],"execution_count":50},{"cell_type":"code","source":"# !pip uninstall rouge -y  # Remove the incorrect package\n# !pip install rouge-score\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:22:11.530568Z","iopub.execute_input":"2025-03-11T14:22:11.530882Z","iopub.status.idle":"2025-03-11T14:22:11.534231Z","shell.execute_reply.started":"2025-03-11T14:22:11.530858Z","shell.execute_reply":"2025-03-11T14:22:11.533316Z"}},"outputs":[],"execution_count":51},{"cell_type":"code","source":"# from rouge_score import rouge_scorer\n\n# def calculate_custom_rouge_l_evaluation(GT_sentences, predicted_sentences):\n#     scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    \n#     # Compute ROUGE-L recall for each sentence pair\n#     scores = [scorer.score(gt, pred)['rougeL'].recall for gt, pred in zip(GT_sentences, predicted_sentences)]\n    \n#     # Average the scores\n#     avg_rouge_l_recall = sum(scores) / len(scores)\n\n#     return avg_rouge_l_recall\n\n# # Assuming test_cap (GT) and DEiT_gen_cap (Predictions) are lists of sentences\n# custom_rouge_l_score = calculate_custom_rouge_l_evaluation(test_cap, DEiT_gen_cap)\n# print(f\"Custom Rouge-L score: {custom_rouge_l_score:.5f}\")\n","metadata":{"id":"i0A-phZ4jZ4_","outputId":"30a4af2e-5f01-4ab3-f092-76d204e50301","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:22:13.196983Z","iopub.execute_input":"2025-03-11T14:22:13.197322Z","iopub.status.idle":"2025-03-11T14:22:13.200834Z","shell.execute_reply.started":"2025-03-11T14:22:13.197293Z","shell.execute_reply":"2025-03-11T14:22:13.200102Z"}},"outputs":[],"execution_count":52},{"cell_type":"markdown","source":"### **VITGPT2**","metadata":{"id":"QmlCl0u1CFuh"}},{"cell_type":"code","source":"# Initialize GPT2 tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\nEmbedding_model = GPT2Model.from_pretrained('gpt2')\n\n# Example predicted and actual sentences\npred_cap = test_cap  # Replace with your predicted sentences\ngen_cap = ViT_gen_cap         # Replace with your actual sentences\n\n# Tokenize predicted and actual sentences\npred_tokens = [tokenizer.encode(sentence, return_tensors=\"pt\", padding=False, truncation=True) for sentence in pred_cap]\ngen_tokens = [tokenizer.encode(sentence, return_tensors=\"pt\", padding=False, truncation=True) for sentence in gen_cap]\n\n# Obtain embeddings for predicted and actual sentences\nwith torch.no_grad():\n    pred_outputs = [Embedding_model(pred_token) for pred_token in pred_tokens]\n    gen_outputs = [Embedding_model(gen_token) for gen_token in gen_tokens]\n\n# Extract embeddings from model outputs\npred_embeddings = [output.last_hidden_state[:, 0, :].numpy() for output in pred_outputs]\ngen_embeddings = [output.last_hidden_state[:, 0, :].numpy() for output in gen_outputs]","metadata":{"id":"s0K_S9v9HFTw","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:37:33.551585Z","iopub.execute_input":"2025-03-11T14:37:33.551935Z","iopub.status.idle":"2025-03-11T14:37:34.900116Z","shell.execute_reply.started":"2025-03-11T14:37:33.551906Z","shell.execute_reply":"2025-03-11T14:37:34.899373Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"cosine_similarity_scores = []\nfor pred_embedding, gen_embedding in zip(pred_embeddings, gen_embeddings):\n    cosine_similarity_score = cosine_similarity(pred_embedding, gen_embedding)[0][0]\n    cosine_similarity_scores.append(cosine_similarity_score)\nprint(\"Skip thought CS: \",np.mean(cosine_similarity_scores))","metadata":{"id":"mZEtK_0aUSfA","outputId":"5d8b2e5b-0677-4a38-efbb-b2fe7db3d26a","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:37:40.125691Z","iopub.execute_input":"2025-03-11T14:37:40.126045Z","iopub.status.idle":"2025-03-11T14:37:40.132922Z","shell.execute_reply.started":"2025-03-11T14:37:40.126019Z","shell.execute_reply":"2025-03-11T14:37:40.132193Z"}},"outputs":[{"name":"stdout","text":"Skip thought CS:  0.9871181\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"np.mean(cosine_similarity_scores)","metadata":{"id":"L3_fONLEUsmu","outputId":"42e20838-59a2-4256-9540-fb4834b73b9f","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:37:49.375246Z","iopub.execute_input":"2025-03-11T14:37:49.375546Z","iopub.status.idle":"2025-03-11T14:37:49.381149Z","shell.execute_reply.started":"2025-03-11T14:37:49.375524Z","shell.execute_reply":"2025-03-11T14:37:49.380253Z"}},"outputs":[{"execution_count":50,"output_type":"execute_result","data":{"text/plain":"0.9871181"},"metadata":{}}],"execution_count":50},{"cell_type":"code","source":"greedy_matching_scores = []\nfor pred_embedding, gen_embedding in zip(pred_embeddings, gen_embeddings):\n    greedy_matching_score = np.corrcoef(pred_embedding,gen_embedding)\n    greedy_matching_scores.append(greedy_matching_score)\nprint(\"Greedy matching scores between predicted and actual sentences:\", np.mean(greedy_matching_scores))","metadata":{"id":"eSTri_KhUs5l","outputId":"db2d6385-ded8-4a3d-9efc-e2260082aebc","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:37:53.238935Z","iopub.execute_input":"2025-03-11T14:37:53.239258Z","iopub.status.idle":"2025-03-11T14:37:53.245281Z","shell.execute_reply.started":"2025-03-11T14:37:53.239234Z","shell.execute_reply":"2025-03-11T14:37:53.244328Z"}},"outputs":[{"name":"stdout","text":"Greedy matching scores between predicted and actual sentences: 0.9935609442957658\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"pred_extrema_embeddings = [[np.min(embedding), np.max(embedding)] for embedding in pred_embeddings]\ngen_extrema_embeddings = [[np.min(embedding), np.max(embedding)] for embedding in gen_embeddings]\nres = []\nfor v1,v2 in zip(pred_extrema_embeddings, gen_extrema_embeddings):\n  v1 = np.array(v1)\n  v2 = np.array(v2)\n  res.append(cosine_similarity(v1.reshape(1,-1), v2.reshape(1,-1))[0][0])\n  # break\nprint(\"Vector Extrema: \",np.mean(res))","metadata":{"id":"lwBZiz2oUu-R","outputId":"13881c96-8361-499e-baa4-e691c3cccd34","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:37:56.203464Z","iopub.execute_input":"2025-03-11T14:37:56.203756Z","iopub.status.idle":"2025-03-11T14:37:56.211779Z","shell.execute_reply.started":"2025-03-11T14:37:56.203733Z","shell.execute_reply":"2025-03-11T14:37:56.210884Z"}},"outputs":[{"name":"stdout","text":"Vector Extrema:  0.9964092\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"\n# from nltk.translate.bleu_score import corpus_bleu\n\n# def calculate_bleu_evaluation(GT_sentences, predicted_sentences):\n#     BLEU_1 = corpus_bleu(GT_sentences, predicted_sentences, weights=w1)\n#     BLEU_2 = corpus_bleu(GT_sentences, predicted_sentences, weights=w2)\n#     BLEU_3 = corpus_bleu(GT_sentences, predicted_sentences, weights=w3)\n#     BLEU_4 = corpus_bleu(GT_sentences, predicted_sentences, weights=w4)\n\n#     return BLEU_1,BLEU_2,BLEU_3,BLEU_4\n\n\n\n# bleu1_scores, bleu2_scores, bleu3_scores, bleu4_scores = calculate_bleu_evaluation(ViT_gen_cap, test_cap)\n# bleu1_scores = np.mean(bleu1_scores)\n# bleu2_scores = np.mean(bleu2_scores)\n# bleu3_scores = np.mean(bleu3_scores)\n# bleu4_scores = np.mean(bleu4_scores)\n\n\n# print(f\"BLEU-1 score: {bleu1_scores:.5f}\")\n# print(f\"BLEU-2 score: {bleu2_scores:.5f}\")\n# print(f\"BLEU-3 score: {bleu3_scores:.5f}\")\n# print(f\"BLEU-4 score: {bleu4_scores:.5f}\")\n","metadata":{"id":"URoD3FGoV28O","outputId":"a8e6257d-9493-4762-aa35-386f2d74be25","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:22:23.053721Z","iopub.execute_input":"2025-03-11T14:22:23.054011Z","iopub.status.idle":"2025-03-11T14:22:23.057417Z","shell.execute_reply.started":"2025-03-11T14:22:23.053990Z","shell.execute_reply":"2025-03-11T14:22:23.056633Z"}},"outputs":[],"execution_count":58},{"cell_type":"code","source":"# from rouge_score import rouge_scorer\n\n# def calculate_custom_rouge_l_evaluation(GT_sentences, predicted_sentences):\n#     scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    \n#     # Compute ROUGE-L recall for each sentence pair\n#     scores = [scorer.score(gt, pred)['rougeL'].recall for gt, pred in zip(GT_sentences, predicted_sentences)]\n    \n#     # Average the scores\n#     avg_rouge_l_recall = sum(scores) / len(scores)\n\n#     return avg_rouge_l_recall\n\n# # Assuming test_cap (GT) and DEiT_gen_cap (Predictions) are lists of sentences\n# custom_rouge_l_score = calculate_custom_rouge_l_evaluation(test_cap, ViT_gen_cap)\n# print(f\"Custom Rouge-L score: {custom_rouge_l_score:.5f}\")\n","metadata":{"id":"kXj4eyaIja-H","outputId":"d01bd017-5000-4915-c6db-59aaa057d4a5","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:22:23.983545Z","iopub.execute_input":"2025-03-11T14:22:23.983829Z","iopub.status.idle":"2025-03-11T14:22:23.987361Z","shell.execute_reply.started":"2025-03-11T14:22:23.983807Z","shell.execute_reply":"2025-03-11T14:22:23.986433Z"}},"outputs":[],"execution_count":59},{"cell_type":"markdown","source":"### **BEITGPT2**","metadata":{"id":"DjEfxMrlCJJv"}},{"cell_type":"code","source":"# Initialize GPT2 tokenizer and model\ntokenizer = GPT2Tokenizer.from_pretrained('gpt2')\ntokenizer.pad_token = tokenizer.eos_token\nEmbedding_model = GPT2Model.from_pretrained('gpt2')\n\n# Example predicted and actual sentences\npred_cap = test_cap  # Replace with your predicted sentences\ngen_cap = BEiT_gen_cap         # Replace with your actual sentences\n\n# Tokenize predicted and actual sentences\npred_tokens = [tokenizer.encode(sentence, return_tensors=\"pt\", padding=False, truncation=True) for sentence in pred_cap]\ngen_tokens = [tokenizer.encode(sentence, return_tensors=\"pt\", padding=False, truncation=True) for sentence in gen_cap]\n\n# Obtain embeddings for predicted and actual sentences\nwith torch.no_grad():\n    pred_outputs = [Embedding_model(pred_token) for pred_token in pred_tokens]\n    gen_outputs = [Embedding_model(gen_token) for gen_token in gen_tokens]\n\n# Extract embeddings from model outputs\npred_embeddings = [output.last_hidden_state[:, 0, :].numpy() for output in pred_outputs]\ngen_embeddings = [output.last_hidden_state[:, 0, :].numpy() for output in gen_outputs]","metadata":{"id":"-FToobozU15K","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:38:01.183594Z","iopub.execute_input":"2025-03-11T14:38:01.183959Z","iopub.status.idle":"2025-03-11T14:38:02.449849Z","shell.execute_reply.started":"2025-03-11T14:38:01.183931Z","shell.execute_reply":"2025-03-11T14:38:02.448895Z"}},"outputs":[],"execution_count":53},{"cell_type":"code","source":"cosine_similarity_scores = []\nfor pred_embedding, gen_embedding in zip(pred_embeddings, gen_embeddings):\n    cosine_similarity_score = cosine_similarity(pred_embedding, gen_embedding)[0][0]\n    cosine_similarity_scores.append(cosine_similarity_score)\nprint(\"Skip thought CS: \",np.mean(cosine_similarity_scores))","metadata":{"id":"_Vvh9EbjU7qm","outputId":"96d10774-c932-4eee-e73b-5577b18b5525","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:38:04.902971Z","iopub.execute_input":"2025-03-11T14:38:04.903306Z","iopub.status.idle":"2025-03-11T14:38:04.909892Z","shell.execute_reply.started":"2025-03-11T14:38:04.903279Z","shell.execute_reply":"2025-03-11T14:38:04.909074Z"}},"outputs":[{"name":"stdout","text":"Skip thought CS:  0.9911761\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"greedy_matching_scores = []\nfor pred_embedding, gen_embedding in zip(pred_embeddings, gen_embeddings):\n    greedy_matching_score = np.corrcoef(pred_embedding,gen_embedding)\n    greedy_matching_scores.append(greedy_matching_score)\nprint(\"Greedy matching scores between predicted and actual sentences:\", np.mean(greedy_matching_scores))","metadata":{"id":"51QXpLIPU_-T","outputId":"e5356434-f32f-46c2-ce99-6b740a063897","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:38:07.053669Z","iopub.execute_input":"2025-03-11T14:38:07.054007Z","iopub.status.idle":"2025-03-11T14:38:07.059887Z","shell.execute_reply.started":"2025-03-11T14:38:07.053979Z","shell.execute_reply":"2025-03-11T14:38:07.059177Z"}},"outputs":[{"name":"stdout","text":"Greedy matching scores between predicted and actual sentences: 0.9955875955051011\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"pred_extrema_embeddings = [[np.min(embedding), np.max(embedding)] for embedding in pred_embeddings]\ngen_extrema_embeddings = [[np.min(embedding), np.max(embedding)] for embedding in gen_embeddings]\nres = []\nfor v1,v2 in zip(pred_extrema_embeddings, gen_extrema_embeddings):\n  v1 = np.array(v1)\n  v2 = np.array(v2)\n  res.append(cosine_similarity(v1.reshape(1,-1), v2.reshape(1,-1))[0][0])\n  # break\nprint(\"Vector Extrema: \",np.mean(res))","metadata":{"id":"G6JYmf5_VCAG","outputId":"e7b1db1e-2fb9-4166-a8bd-c68b493f0b71","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:38:09.426359Z","iopub.execute_input":"2025-03-11T14:38:09.426690Z","iopub.status.idle":"2025-03-11T14:38:09.433724Z","shell.execute_reply.started":"2025-03-11T14:38:09.426662Z","shell.execute_reply":"2025-03-11T14:38:09.432899Z"}},"outputs":[{"name":"stdout","text":"Vector Extrema:  0.9990377\n","output_type":"stream"}],"execution_count":56},{"cell_type":"code","source":"\n# from nltk.translate.bleu_score import corpus_bleu\n\n# def calculate_bleu_evaluation(GT_sentences, predicted_sentences):\n#     BLEU_1 = corpus_bleu(GT_sentences, predicted_sentences, weights=w1)\n#     BLEU_2 = corpus_bleu(GT_sentences, predicted_sentences, weights=w2)\n#     BLEU_3 = corpus_bleu(GT_sentences, predicted_sentences, weights=w3)\n#     BLEU_4 = corpus_bleu(GT_sentences, predicted_sentences, weights=w4)\n\n#     return BLEU_1,BLEU_2,BLEU_3,BLEU_4\n\n\n\n# bleu1_scores, bleu2_scores, bleu3_scores, bleu4_scores = calculate_bleu_evaluation(BEiT_gen_cap, test_cap)\n\n# bleu1_scores = np.mean(bleu1_scores)\n# bleu2_scores = np.mean(bleu2_scores)\n# bleu3_scores = np.mean(bleu3_scores)\n# bleu4_scores = np.mean(bleu4_scores)\n\n\n# print(f\"BLEU-1 score: {bleu1_scores:.5f}\")\n# print(f\"BLEU-2 score: {bleu2_scores:.5f}\")\n# print(f\"BLEU-3 score: {bleu3_scores:.5f}\")\n# print(f\"BLEU-4 score: {bleu4_scores:.5f}\")\n","metadata":{"id":"zjaFzmlDV5S0","outputId":"bf2ad33b-d335-448b-b75a-6458c521e076","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:22:32.540614Z","iopub.execute_input":"2025-03-11T14:22:32.540941Z","iopub.status.idle":"2025-03-11T14:22:32.544785Z","shell.execute_reply.started":"2025-03-11T14:22:32.540916Z","shell.execute_reply":"2025-03-11T14:22:32.543896Z"}},"outputs":[],"execution_count":64},{"cell_type":"code","source":"# from rouge_score import rouge_scorer\n\n# def calculate_custom_rouge_l_evaluation(GT_sentences, predicted_sentences):\n#     scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n    \n#     # Compute ROUGE-L recall for each sentence pair\n#     scores = [scorer.score(gt, pred)['rougeL'].recall for gt, pred in zip(GT_sentences, predicted_sentences)]\n    \n#     # Average the scores\n#     avg_rouge_l_recall = sum(scores) / len(scores)\n\n#     return avg_rouge_l_recall\n\n# # Assuming test_cap (GT) and DEiT_gen_cap (Predictions) are lists of sentences\n# custom_rouge_l_score = calculate_custom_rouge_l_evaluation(test_cap, BEiT_gen_cap)\n# print(f\"Custom Rouge-L score: {custom_rouge_l_score:.5f}\")\n","metadata":{"id":"e_NzPFzejcJm","outputId":"7c122dcb-2e08-4e8a-a2b7-82d85b085bd5","trusted":true,"execution":{"iopub.status.busy":"2025-03-11T14:22:33.511230Z","iopub.execute_input":"2025-03-11T14:22:33.511555Z","iopub.status.idle":"2025-03-11T14:22:33.514977Z","shell.execute_reply.started":"2025-03-11T14:22:33.511529Z","shell.execute_reply":"2025-03-11T14:22:33.514226Z"}},"outputs":[],"execution_count":65},{"cell_type":"code","source":"# headers = [\"Epoch\",\"Train loss\",\"Train Perplexity\",\"Validation Loss\",\"Validation Perplexity\"]\n# Bdf = pd.read_csv(\"/content/drive/MyDrive/X_ray_Report_generation/DeitRes.csv\",names=headers)\n# Ddf = pd.read_csv(\"/content/drive/MyDrive/X_ray_Report_generation/BEiTRes.csv\",names=headers)\n# Vdf = pd.read_csv(\"/content/drive/MyDrive/X_ray_Report_generation/ViTRes.csv\",names=headers)","metadata":{"id":"QiQOF9J8V8bC"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Vdf[\"Epoch\"]=Vdf[\"Epoch\"]+1\n# Bdf[\"Epoch\"]=Bdf[\"Epoch\"]+1\n# Ddf[\"Epoch\"]=Ddf[\"Epoch\"]+1","metadata":{"id":"NrwFlmFyyjxS"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# # Plotting Loss\n# import matplotlib.pyplot as plt\n# from matplotlib.ticker import MaxNLocator\n# plt.figure(figsize=(10, 5))\n\n# # Loss\n# plt.subplot(1, 2, 1)\n# plt.gca().yaxis.set_major_locator(MaxNLocator(integer=True))\n# plt.plot(Bdf['Epoch'], Bdf['Train loss'], label='DeitGPT2 Train Loss',marker='o')\n# plt.plot(Ddf['Epoch'], Ddf['Train loss'], label='BEiTGPT2 Train Loss',marker='o')\n# plt.plot(Vdf['Epoch'], Vdf['Train loss'], label='ViTGPT2 Train Loss',marker='o')\n# plt.title('Training Loss')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n\n# plt.legend()\n\n# # Validation Loss\n# plt.subplot(1, 2, 2)\n# plt.yticks([i for i in range(0, 20, 2)])\n# plt.plot(Bdf['Epoch'], Bdf['Validation Loss'], label='DeitGPT2 Validation Loss',marker='o')\n# plt.plot(Ddf['Epoch'], Ddf['Validation Loss'], label='BEiTGPT2 Validation Loss',marker='o')\n# plt.plot(Vdf['Epoch'], Vdf['Validation Loss'], label='ViTGPT2  Validation Loss',marker='o')\n# plt.title('Validation Loss')\n\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.legend()\n\n# plt.tight_layout()\n# plt.savefig(\"loss_plot.png\",dpi=300)\n# plt.show()\n","metadata":{"id":"FI_-HMjyamU3","outputId":"171812ee-dbb8-4986-d0ce-0f4776d6a384"},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import os \n# os.chdir(r'/kaggle/working')","metadata":{"id":"qB4x1mlSdCYU","trusted":true,"execution":{"iopub.status.busy":"2025-03-05T08:44:58.854023Z","iopub.execute_input":"2025-03-05T08:44:58.854331Z","iopub.status.idle":"2025-03-05T08:44:58.858349Z","shell.execute_reply.started":"2025-03-05T08:44:58.854306Z","shell.execute_reply":"2025-03-05T08:44:58.857309Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# from IPython.display import FileLink\n# FileLink(r'/kaggle/working/DEiTcaptioner_new.pt')\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-05T08:45:26.798367Z","iopub.execute_input":"2025-03-05T08:45:26.798644Z","iopub.status.idle":"2025-03-05T08:45:26.804359Z","shell.execute_reply.started":"2025-03-05T08:45:26.798623Z","shell.execute_reply":"2025-03-05T08:45:26.803524Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}